{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec Overview\n",
    "Word2Vec is a popular model used to learn word embeddings, where words are represented as dense vectors in a continuous vector space. The two most common algorithms for training Word2Vec models are Skip-gram and Continuous Bag of Words (CBOW). Let's go over each of these and their key differences.\n",
    "\n",
    "##### Word2Vec Algorithms\n",
    "- Skip-gram Model:\n",
    "\n",
    "    - Objective: Given a word (the center word), the goal is to predict its surrounding context words (within a defined window).\n",
    "    - Training Mechanism: For each word in a sentence, the model is trained to predict the surrounding words within a fixed window. The idea is that if a word appears in a particular context, it has similar meanings or uses.\n",
    "    - Best For: This model is particularly effective when the corpus is small and there are rare words. Skip-gram tries to predict words given a central word, and it works well when trying to capture more fine-grained semantic meanings.\n",
    "\n",
    "- CBOW Model (Continuous Bag of Words):\n",
    "\n",
    "    - Objective: Given a set of context words (surrounding words), the goal is to predict the target word (the center word).\n",
    "    - Training Mechanism: In this model, the surrounding context words are used to predict the target word. This means that for a given context, the model tries to guess the word that fits into that context.\n",
    "    - Best For: This model works better when the corpus is large and there are frequent words. CBOW captures word meanings more directly from the surrounding context.\n",
    "\n",
    "##### Key Differences Between Skip-gram and CBOW\n",
    "\n",
    "| Aspect               | Skip-gram                                           | CBOW                                                |\n",
    "|----------------------|-----------------------------------------------------|-----------------------------------------------------|\n",
    "| **Training Goal**     | Predict surrounding context words from a given center word. | Predict a center word from surrounding context words. |\n",
    "| **Speed**             | Slower (especially on large corpora)                | Faster                                              |\n",
    "| **Use Case**          | Better for smaller datasets, capturing rare words. | Better for larger datasets, handling frequent words more effectively. |\n",
    "| **Memory Usage**      | Higher memory consumption                          | Lower memory consumption                            |\n",
    "\n",
    "\n",
    "##### How Are Word2Vec Models Trained?\n",
    "\n",
    "Both Skip-gram and CBOW are trained using neural networks, specifically a shallow neural network with an input layer, a hidden layer, and an output layer. The weights of the hidden layer represent the word embeddings. The process can be broken down as follows:\n",
    "\n",
    "- **Input Layer**: The word is converted into a one-hot encoded vector.\n",
    "- **Hidden Layer**: This is the layer where the word vectors are learned. It has a size corresponding to the desired dimensionality of the word embeddings.\n",
    "- **Output Layer**: \n",
    "  - For **Skip-gram**, it predicts the context words.\n",
    "  - For **CBOW**, it predicts the target word based on context.\n",
    "\n",
    "To optimize these models, the algorithm uses a technique called **negative sampling** or **hierarchical softmax** to make the training more efficient.\n",
    "\n",
    "\n",
    "##### Applications of Word2Vec Beyond Word Embeddings\n",
    "\n",
    "Word2Vec's embeddings can be used in a wide range of tasks beyond just creating word representations:\n",
    "\n",
    "- **Text Classification**: Word embeddings can be used as input features to machine learning models (e.g., SVMs, Logistic Regression) for tasks like sentiment analysis or spam detection.\n",
    "- **Named Entity Recognition (NER)**: Word embeddings can help capture semantic similarities between words, making NER models more accurate in identifying named entities like people, locations, and organizations.\n",
    "- **Machine Translation**: Word2Vec embeddings are used to capture relationships between words in different languages, making it useful in building translation models.\n",
    "- **Document Similarity**: You can calculate the similarity between documents by averaging or combining the embeddings of individual words in the documents.\n",
    "- **Information Retrieval**: Word2Vec can improve search engines by understanding the semantic meaning behind words and phrases, thus allowing better retrieval of relevant results.\n",
    "- **Recommendation Systems**: Word2Vec embeddings can be used in collaborative filtering or content-based recommendation systems to find similar items (e.g., products, movies) based on their descriptions.\n",
    "\n",
    "\n",
    "##### How to Generate Sentence Embeddings Using Word2Vec\n",
    "\n",
    "While Word2Vec directly gives word embeddings, sentence embeddings need to be derived by aggregating the word embeddings of the words in the sentence. The simplest method is to take the **average** of all the word embeddings in the sentence:\n",
    "\n",
    "1. **Step 1**: For each word in the sentence, retrieve its Word2Vec embedding.\n",
    "2. **Step 2**: Average the embeddings to get a fixed-length vector that represents the entire sentence.\n",
    "\n",
    "This method works reasonably well, though more sophisticated techniques (such as **Doc2Vec**, which is designed to learn document-level embeddings) can provide better results.\n",
    "\n",
    "\n",
    "##### Popular Packages for Training Word2Vec Models\n",
    "\n",
    "Several libraries provide implementations of Word2Vec:\n",
    "\n",
    "- **Gensim**:\n",
    "  - One of the most popular libraries for training and using Word2Vec models.\n",
    "  - It has an efficient implementation of Word2Vec for both Skip-gram and CBOW models.\n",
    "\n",
    "  ```python\n",
    "    from gensim.models import Word2Vec\n",
    "    # Training a Word2Vec model using CBOW\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "    ```\n",
    "\n",
    "##### Libraries for Working with Word2Vec\n",
    "\n",
    "##### FastText (by Facebook):\n",
    "A library developed by Facebook that is similar to Word2Vec but also takes subword information into account, which allows it to generate better representations for rare words or out-of-vocabulary words.\n",
    "\n",
    "##### spaCy:\n",
    "Another powerful library for NLP tasks that can work with pre-trained Word2Vec models and can be easily integrated into various NLP pipelines.\n",
    "\n",
    "##### TensorFlow and PyTorch:\n",
    "While these deep learning libraries are not specialized in Word2Vec, you can implement Word2Vec algorithms from scratch or use pre-built models in these frameworks for more flexibility and customization.\n",
    "\n",
    "##### Example of Using Gensim to Train a Word2Vec Model\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"Word2Vec is a great tool for NLP\",\n",
    "    \"Deep learning is a subfield of machine learning\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Retrieve vector for a word\n",
    "vector = model.wv['machine']\n",
    "print(vector)\n",
    "\n",
    "# Get similar words\n",
    "similar_words = model.wv.most_similar('machine', topn=3)\n",
    "print(similar_words)\n",
    "```\n",
    "##### Conclusion\n",
    "\n",
    "- Word2Vec is a powerful tool for generating word embeddings using either **Skip-gram** or **CBOW** approaches, with each having its strengths depending on the context.\n",
    "- Beyond word embeddings, Word2Vec can be used for various tasks such as **text classification**, **NER**, **machine translation**, **recommendation systems**, and more.\n",
    "- **Sentence embeddings** can be derived by averaging word embeddings, though specialized models like **Doc2Vec** might be more effective for capturing document-level semantics.\n",
    "- Popular libraries like **Gensim**, **FastText**, and **spaCy** provide efficient tools for training and using Word2Vec embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
