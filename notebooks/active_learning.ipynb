{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active Learning Algorithm\n",
    "Active learning is a machine learning paradigm where the algorithm can query a user or an oracle to obtain labels for new data points. The goal is to achieve high performance with fewer labeled instances by strategically selecting which data points to label.\n",
    "\n",
    "#### Use Cases for Active Learning\n",
    "- Image Classification: In scenarios where labeling images is expensive or time-consuming, active learning can help focus on uncertain samples to improve the model's accuracy efficiently.\n",
    "\n",
    "- Natural Language Processing: Active learning can be applied in tasks such as sentiment analysis or named entity recognition, where labeling text can be subjective and labor-intensive.\n",
    "\n",
    "- Medical Diagnosis: In medical imaging, where experts are needed to label images, active learning can help prioritize which images to review.\n",
    "\n",
    "- Anomaly Detection: Active learning can be used to identify and label anomalous data points that may be rare and critical for training robust models.\n",
    "\n",
    "- Recommender Systems: In scenarios where user feedback is limited, active learning can be employed to gather relevant data points for better recommendations.\n",
    "\n",
    "#### Generating Logical Data for Active Learning\n",
    "We can generate synthetic logical data, such as a set of binary features with a binary target variable, to simulate a classification problem. Here’s how to create such data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random logical data\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "num_features = 5\n",
    "\n",
    "# Random binary features\n",
    "X = np.random.randint(0, 2, size=(num_samples, num_features))\n",
    "\n",
    "# Target variable (logical AND operation)\n",
    "# For instance, the target could be 1 if all features are 1, otherwise 0\n",
    "y = np.all(X, axis=1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Active Learning from Scratch Using NumPy\n",
    "Below is a simple implementation of an active learning algorithm using NumPy. We'll use a basic model (e.g., logistic regression) and implement a pool-based active learning strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query indices: [51 63 80 58 44], Uncertainty: [0.17057349 0.17198388 0.17198388 0.20477341 0.20477341]\n",
      "Updated labeled set size: 15, Pool size: 0\n",
      "Iteration 1, Accuracy: 0.9800\n",
      "Not enough samples in the pool to query.\n",
      "No new samples to update.\n",
      "Iteration 2, Accuracy: 0.9800\n",
      "Not enough samples in the pool to query.\n",
      "No new samples to update.\n",
      "Iteration 3, Accuracy: 0.9800\n",
      "Not enough samples in the pool to query.\n",
      "No new samples to update.\n",
      "Iteration 4, Accuracy: 0.9800\n",
      "Not enough samples in the pool to query.\n",
      "No new samples to update.\n",
      "Iteration 5, Accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class ActiveLearner:\n",
    "    def __init__(self, X_pool, y_pool, initial_size=10, query_size=5):\n",
    "        # Initialize the Active Learner with given parameters\n",
    "        self.X_pool = X_pool  # Pool of unlabeled samples\n",
    "        self.y_pool = y_pool  # Corresponding labels for the pool\n",
    "        self.initial_size = initial_size  # Number of samples to initially label\n",
    "        self.query_size = query_size  # Number of samples to query in each iteration\n",
    "        self.model = LogisticRegression()  # Logistic Regression model for classification\n",
    "        \n",
    "        # Select the initial labeled dataset\n",
    "        self.X_labeled = self.X_pool[:self.initial_size]\n",
    "        self.y_labeled = self.y_pool[:self.initial_size]\n",
    "        \n",
    "        # Train the model on the initial labeled data\n",
    "        self.model.fit(self.X_labeled, self.y_labeled)\n",
    "        \n",
    "        # Remove the initial labeled data from the pool\n",
    "        self.X_pool = self.X_pool[self.initial_size:]\n",
    "        self.y_pool = self.y_pool[self.initial_size:]\n",
    "\n",
    "    def query(self):\n",
    "        # Check if there are enough samples in the pool to query\n",
    "        if len(self.X_pool) < self.query_size:\n",
    "            print(\"Not enough samples in the pool to query.\")\n",
    "            return self.X_pool, self.y_pool, np.arange(len(self.X_pool))  # Return whatever is available\n",
    "\n",
    "        # Get predicted probabilities for the current model on the unlabeled pool\n",
    "        probs = self.model.predict_proba(self.X_pool)\n",
    "        \n",
    "        # Calculate uncertainty as 1 - max probability (higher values indicate more uncertainty)\n",
    "        uncertainty = 1 - np.max(probs, axis=1)\n",
    "        \n",
    "        # Select the indices of the most uncertain samples\n",
    "        query_indices = np.argsort(uncertainty)[-self.query_size:]  # Get indices of the top uncertain samples\n",
    "        print(f\"Query indices: {query_indices}, Uncertainty: {uncertainty[query_indices]}\")  # Debugging info\n",
    "        \n",
    "        # Return the samples and their true labels\n",
    "        return self.X_pool[query_indices], self.y_pool[query_indices], query_indices\n",
    "\n",
    "    def update(self, X_new, y_new):\n",
    "        # Ensure there are new samples to update\n",
    "        if len(X_new) == 0 or len(y_new) == 0:\n",
    "            print(\"No new samples to update.\")\n",
    "            return  # Early return if no new samples are available\n",
    "\n",
    "        # Check the labels in new samples to ensure we are not just getting one class\n",
    "        unique_labels = np.unique(y_new)\n",
    "        if len(unique_labels) < 2:\n",
    "            print(\"New samples must contain both classes for the model to learn.\")\n",
    "            return  # Exit if new samples do not provide both classes\n",
    "        \n",
    "        # Append the newly labeled samples to the existing labeled dataset\n",
    "        self.X_labeled = np.vstack((self.X_labeled, X_new))\n",
    "        self.y_labeled = np.concatenate((self.y_labeled, y_new))\n",
    "        \n",
    "        # Create a mask to identify new samples in the pool\n",
    "        mask = np.isin(self.X_pool, X_new).any(axis=1)  # This checks if each row in X_pool matches any row in X_new\n",
    "        self.X_pool = self.X_pool[~mask]  # Keep only samples not in X_new\n",
    "        self.y_pool = self.y_pool[~mask]  # Keep only labels not in y_new\n",
    "        \n",
    "        # Print the updated sizes of the labeled dataset and the remaining pool\n",
    "        print(f\"Updated labeled set size: {len(self.X_labeled)}, Pool size: {len(self.X_pool)}\")\n",
    "        \n",
    "        # Retrain the model with the updated labeled dataset\n",
    "        self.model.fit(self.X_labeled, self.y_labeled)\n",
    "\n",
    "    def get_performance(self, X_test, y_test):\n",
    "        # Make predictions on the test set\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        # Calculate and return the accuracy score\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Simulate some random logical data for testing\n",
    "num_features = 5  # Number of features for each sample\n",
    "num_samples = 100  # Total number of samples\n",
    "X = np.random.randint(0, 2, size=(num_samples, num_features))  # Random binary features\n",
    "y = np.all(X, axis=1).astype(int)  # Labels: 1 if all features are 1, else 0\n",
    "\n",
    "# Ensure we have at least one of each class\n",
    "if np.unique(y).size < 2:\n",
    "    raise ValueError(\"Generated labels do not contain both classes.\")\n",
    "\n",
    "# Initialize active learner\n",
    "active_learner = ActiveLearner(X, y)\n",
    "\n",
    "# Sample test set for evaluation (ensure it contains both classes)\n",
    "X_test = np.random.randint(0, 2, size=(200, num_features))\n",
    "y_test = np.all(X_test, axis=1).astype(int)  # Same labeling logic as above\n",
    "\n",
    "# Ensure test set has both classes\n",
    "if np.unique(y_test).size < 2:\n",
    "    raise ValueError(\"Generated test labels do not contain both classes.\")\n",
    "\n",
    "# Simulate active learning process\n",
    "for iteration in range(5):  # Run for 5 iterations\n",
    "    X_query, y_query, query_indices = active_learner.query()  # Query the most uncertain samples\n",
    "    active_learner.update(X_query, y_query)  # Update the model with new labels\n",
    "    accuracy = active_learner.get_performance(X_test, y_test)  # Evaluate model performance\n",
    "    print(f\"Iteration {iteration + 1}, Accuracy: {accuracy:.4f}\")  # Print the accuracy for this iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Key Parts\n",
    "- Imports:\n",
    "\n",
    "    - The necessary libraries (sklearn for the logistic regression model and accuracy score, and numpy for numerical operations) are imported at the start.\n",
    "- Initialization (__init__ method):\n",
    "\n",
    "    - X_pool and y_pool: The pools of unlabeled data and their true labels are stored for querying.\n",
    "    - initial_size and query_size: Define how many samples to use for initial training and how many to query in each iteration, respectively.\n",
    "    - A logistic regression model is instantiated, and the initial labeled data is selected and fitted to the model. This is essential to get the first iteration started.\n",
    "- Query Method:\n",
    "\n",
    "    - The query method calculates the probabilities of the unlabeled data. The uncertainty is assessed by determining which predictions are the least confident (i.e., probabilities close to 0.5). This is crucial for active learning because the goal is to label the most informative data points.\n",
    "- Update Method:\n",
    "\n",
    "    - After querying, the update method adds the newly labeled data to the existing labeled dataset and retrains the model. This is important for improving the model's performance iteratively with new data.\n",
    "- Performance Evaluation:\n",
    "\n",
    "    - The get_performance method measures the accuracy of the model on a separate test set. Evaluating the model’s performance after each iteration helps track its improvement.\n",
    "- Simulation Loop:\n",
    "\n",
    "    - The loop simulates the active learning process for a specified number of iterations, printing the accuracy after each iteration to monitor progress. This provides a practical demonstration of how active learning refines the model over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use Active Learning and When Not to Use It\n",
    "- When to Use Active Learning:\n",
    "\n",
    "    - When labeling data is expensive or time-consuming.\n",
    "    - When you have a large pool of unlabeled data but only a limited budget for labeling.\n",
    "    - When the model can benefit from focusing on uncertain samples.\n",
    "    - When working with complex tasks where expert labeling is required.\n",
    "\n",
    "- When Not to Use Active Learning:\n",
    "\n",
    "    - When you have ample labeled data already available.\n",
    "    - When the cost of labeling is low, and traditional supervised learning suffices.\n",
    "    - When your task does not benefit significantly from uncertainty sampling.\n",
    "\n",
    "##### What is the Loss Function\n",
    "In active learning, the loss function typically depends on the model being used. For logistic regression, the loss function is usually the binary cross-entropy loss, defined as:\n",
    "\n",
    "$Loss = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $y_i$: True label\n",
    "* $\\hat{y}_i$: Predicted probability of the positive class\n",
    "* $n$: Number of samples\n",
    "\n",
    "#### How to Optimize the Algorithm\n",
    "- Model Selection: Use more advanced models that can better capture complex relationships in the data.\n",
    "\n",
    "- Smart Query Strategies: Experiment with different query strategies (e.g., uncertainty sampling, query by committee) to find the most effective one for your specific use case.\n",
    "\n",
    "- Dynamic Query Size: Adjust the size of the query based on the confidence of the model or the remaining pool of unlabeled data.\n",
    "\n",
    "- Budget Management: Monitor the labeling budget closely and prioritize data points that will give the most significant performance gains.\n",
    "\n",
    "- Ensemble Methods: Use ensemble methods to improve the robustness of predictions and uncertainty estimates.\n",
    "\n",
    "- Performance Monitoring: Continuously evaluate the model's performance on a validation set to ensure that active learning is improving the model effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
