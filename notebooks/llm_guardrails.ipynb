{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### course-https://learn.deeplearning.ai/courses/safe-and-reliable-ai-via-guardrails/lesson/1/introduction\n",
    "\n",
    "**What is RAG and Guardrail?**\n",
    "\n",
    "* **RAG (Retrieval Augmented Generation):** A technique where LLMs access and process information from external knowledge sources to improve the quality and relevance of their responses. \n",
    "* **Guardrail:** A safety mechanism or validation tool that ensures AI applications, particularly those using LLMs, adhere to specific rules and operate within predefined boundaries. \n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Challenges with LLMs:**\n",
    "  * Unpredictable and variable outputs\n",
    "  * Difficulty in ensuring reliability and compliance \n",
    "* **Role of Guardrails:**\n",
    "  * Mitigating LLM failures like hallucinations and information disclosure \n",
    "  * Enforcing specific rules and guidelines \n",
    "  * Improving reliability and trustworthiness of LLM-powered applications\n",
    "* **Guardrail Implementation:**\n",
    "  * Checking input and output of LLMs\n",
    "  * Using validators to enforce rules\n",
    "  * Employing techniques like NLI to ensure factual accuracy\n",
    "  * Building custom guardrails for specific needs\n",
    "* **Benefits of Guardrails:**\n",
    "  * Increased confidence in LLM-based systems\n",
    "  * Reduced risk of negative outcomes\n",
    "  * Enhanced ability to deploy LLM-powered applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Failure Modes in RAG Applications**\n",
    "\n",
    "1. **Factual Inaccuracy:**\n",
    "   * **Hallucinations:** The model generates text that is not grounded in the provided information.\n",
    "   * **Misinterpretations:** The model misunderstands the query or the retrieved information.\n",
    "\n",
    "2. **Lack of Relevance:**\n",
    "   * **Off-topic Responses:** The model generates responses that are irrelevant to the query.\n",
    "   * **Insufficient Information:** The model fails to provide sufficient information to answer the query.\n",
    "\n",
    "3. **Bias and Stereotypes:**\n",
    "   * **Perpetuating Bias:** The model reinforces existing biases present in the training data.\n",
    "   * **Stereotypical Responses:** The model generates responses that are stereotypical or discriminatory.\n",
    "\n",
    "4. **Security and Privacy Risks:**\n",
    "   * **Data Leakage:** The model may inadvertently expose sensitive information from the knowledge base.\n",
    "   * **Malicious Use:** The model may be used to generate harmful or misleading content.\n",
    "\n",
    "5. **Overreliance on the Knowledge Base:**\n",
    "   * **Limited Creativity:** The model may become overly reliant on the knowledge base, limiting its ability to generate novel or creative responses.\n",
    "   * **Sensitivity to Knowledge Base Quality:** The model's performance can be significantly impacted by the quality and relevance of the knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Guardrails?**\n",
    "\n",
    "Guardrails are safety mechanisms and validation tools designed to ensure that AI applications, particularly those using large language models (LLMs), adhere to specific rules and operate within predefined boundaries. They help mitigate risks and ensure the responsible and ethical use of AI. \n",
    "\n",
    "**Key functions of guardrails:**\n",
    "\n",
    "* **Preventing Hallucinations:** Guardrails can help prevent LLMs from generating false or misleading information.\n",
    "* **Enforcing Factual Accuracy:** They can ensure that the LLM's responses are grounded in factual data.\n",
    "* **Mitigating Bias:** Guardrails can help reduce bias in the LLM's outputs.\n",
    "* **Protecting Privacy:** They can help protect sensitive information by preventing the LLM from disclosing private data.\n",
    "* **Controlling Tone and Style:** Guardrails can help maintain a consistent tone and style in the LLM's responses.\n",
    "\n",
    "By implementing guardrails, developers can build more reliable, safe, and ethical AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
