{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Code Architecture\n",
    "\n",
    "The Transformer architecture, introduced in the seminal 2017 paper \"Attention is All You Need,\" has transformed NLP by focusing on self-attention mechanisms rather than recurrent neural networks (RNNs). Here's a breakdown of the Transformer’s core components:\n",
    "\n",
    "- Encoder-Decoder Structure: The Transformer is composed of stacked encoder and decoder layers.\n",
    "\n",
    "    - Encoder: The encoder processes the input sequence, generating a set of representations.\n",
    "    - Decoder: The decoder uses these representations to generate the output sequence.\n",
    "- Self-Attention Mechanism: Self-attention computes the relationships between elements in a sequence, capturing dependencies regardless of their distance.\n",
    "\n",
    "- Positional Encoding: Since Transformers lack recurrence, positional encodings add sequence information to the inputs.\n",
    "\n",
    "- Multi-Head Attention: Each attention head captures different aspects of relationships, enriching the learned representations.\n",
    "\n",
    "- Feed-Forward Network (FFN): FFNs are applied after each attention layer, with activation functions and dropout for non-linearity.\n",
    "\n",
    "- Layer Normalization and Residual Connections: Residual connections and normalization stabilize and improve the training of the deep network.\n",
    "\n",
    "#### Explanation of Terms in the Original Paper\n",
    "- Attention: Mechanism to focus on different parts of the sequence, assigning weights to relevant parts.\n",
    "- Scaled Dot-Product Attention: Calculates attention by computing a scaled dot-product between query and key matrices, then applies a softmax to get weights.\n",
    "- Multi-Head Attention: Multiple attention heads to capture diverse relationships within the sequence.\n",
    "- Query (Q), Key (K), Value (V): Matrices derived from the input that help compute attention weights and apply them to obtain relevant information.\n",
    "- Positional Encoding: A way to encode the order of tokens since Transformer lacks sequence awareness.\n",
    "- Feed-Forward Neural Network (FFN): A fully connected network applied after the attention layer.\n",
    "- Residual Connection: Adds the output of a previous layer to the next layer’s output to improve information flow.\n",
    "- Layer Normalization: Stabilizes training by normalizing inputs to each layer.\n",
    "\n",
    "#### Coding the Transformer Architecture\n",
    "Below is a simplified PyTorch implementation of the Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ScaledDotProductAttention computes the attention weights for a given query, key, and value set.\n",
    "# This is a core part of the self-attention mechanism in the Transformer.\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        # Scaling factor for the dot product, helps with gradient stability\n",
    "        self.scale = math.sqrt(d_k)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Compute the attention scores by taking the dot product of Q and the transpose of K\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        # Apply the mask if provided (useful for tasks like language generation)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        # Compute the attention weights by applying softmax to the scores\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        # Multiply the attention weights with the values to get the output\n",
    "        return torch.matmul(attn_weights, V), attn_weights\n",
    "\n",
    "# MultiHeadAttention splits the attention mechanism across multiple heads, allowing the model\n",
    "# to capture different aspects of relationships in the input data.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        # Number of dimensions for each head\n",
    "        self.d_k = d_model // num_heads\n",
    "        # Number of attention heads\n",
    "        self.num_heads = num_heads\n",
    "        # Linear layers to transform the input into query, key, and value matrices\n",
    "        self.qkv_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        # Final linear layer to consolidate multi-head attention output\n",
    "        self.out_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        # Generate Q, K, V for each head, then reshape for attention computation\n",
    "        Q, K, V = [layer(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) \n",
    "                   for layer, x in zip(self.qkv_layers, (Q, K, V))]\n",
    "        # Calculate scaled dot-product attention for each head\n",
    "        attn_output, attn_weights = ScaledDotProductAttention(self.d_k)(Q, K, V, mask)\n",
    "        # Concatenate attention heads and reshape back to original dimensions\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        # Apply the final linear transformation\n",
    "        return self.out_layer(attn_output)\n",
    "\n",
    "# PositionalEncoding adds positional information to each token in the sequence, enabling the model to differentiate\n",
    "# the relative position of tokens, which is crucial since Transformers lack a sequential structure like RNNs.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        # Create a matrix to hold positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Generate position indices for each position\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # Compute sinusoidal values for each position across even and odd dimensions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Register as a buffer so it's not updated during training\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return x\n",
    "\n",
    "# TransformerLayer represents a single layer in the encoder or decoder, which consists of a multi-head attention layer\n",
    "# followed by a feed-forward layer, with layer normalization applied at each step.\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_hidden):\n",
    "        super().__init__()\n",
    "        # Multi-head attention module\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(nn.Linear(d_model, ff_hidden), nn.ReLU(), nn.Linear(ff_hidden, d_model))\n",
    "        # Layer normalization modules to stabilize training\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply multi-head attention and add residual connection\n",
    "        attn_output = self.layernorm1(x + self.attention(x, x, x, mask))\n",
    "        # Apply feed-forward network and add another residual connection\n",
    "        return self.layernorm2(attn_output + self.ff(attn_output))\n",
    "\n",
    "# Transformer combines embedding, positional encoding, and multiple transformer layers\n",
    "# to form a complete encoder or decoder model.\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_hidden, num_layers, max_len, vocab_size):\n",
    "        super().__init__()\n",
    "        # Embedding layer for converting input indices to vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Positional encoding to inject positional information\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        # Stack of transformer layers\n",
    "        self.layers = nn.ModuleList([TransformerLayer(d_model, num_heads, ff_hidden) for _ in range(num_layers)])\n",
    "        # Final output layer that projects the model's outputs to the vocabulary size\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Embed the input and add positional encoding\n",
    "        x = self.pos_encoding(self.embedding(x))\n",
    "        # Pass through each transformer layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # Project to vocabulary size\n",
    "        return self.fc_out(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Code Flow and Key Components\n",
    "- Scaled Dot-Product Attention: Computes attention scores between query, key, and value matrices. The scaling factor helps with gradient stability when handling large dot-products.\n",
    "\n",
    "- Multi-Head Attention: This mechanism enables the model to focus on different parts of the input. The multi-head setup ensures that attention is computed on different projections, allowing the model to capture diverse relationships within the input sequence.\n",
    "\n",
    "- Positional Encoding: Adds positional context to each token in the sequence. Transformers do not inherently recognize sequence order, so sinusoidal positional encodings help the model learn the position of each token, which is critical in sequence processing tasks.\n",
    "\n",
    "- Transformer Layer: Combines multi-head attention and a feed-forward network, with layer normalization and residual connections to stabilize training and enhance information flow. Residual connections add the input of each sub-layer to the output, aiding gradient flow and helping avoid vanishing gradient problems.\n",
    "\n",
    "- Stacked Transformer Model: Combines embeddings, positional encoding, multiple transformer layers, and a final linear layer to create the complete Transformer model. It forms the basis for complex tasks in NLP and is adaptable to other domains like computer vision.\n",
    "\n",
    "#### Importance of Each Part\n",
    "- Attention Mechanisms: Allow the model to focus on important parts of the input sequence, making the Transformer highly effective for long sequences.\n",
    "- Positional Encoding: Addresses the lack of recurrence, essential for sequence data.\n",
    "- Multi-Layer Structure: Enables the model to learn complex patterns through stacked attention and feed-forward layers.\n",
    "- Residual Connections and Normalization: Help with stability during training and prevent gradient issues in deep architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great tutorial on Transformers https://www.youtube.com/watch?v=QCJQG4DuHT0&list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4&index=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
