{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vision Trasformer\n",
    "\n",
    "The Vision Transformer (ViT) introduced the Transformer architecture, initially developed for NLP tasks, to computer vision by treating images as sequences of patches. Below is a breakdown of the ViT architecture, key concepts, a code implementation with detailed comments, and an overview of improvements made since 2017, along with potential future enhancements.\n",
    "\n",
    "#### Vision Transformer (ViT) Architecture Overview\n",
    "The Vision Transformer divides an image into a grid of patches, treats each patch as a “token,” and then applies a Transformer model to these tokens. Here are the main components:\n",
    "\n",
    "- Image Patches: Each image is divided into non-overlapping patches, e.g., a 224x224 image can be divided into 16x16 patches.\n",
    "- Linear Projection of Patches: Each patch is flattened and linearly transformed to create a vector representation.\n",
    "- Class Token: A learnable vector added to the sequence of patches, used for classification tasks.\n",
    "- Positional Encoding: Adds spatial information to each patch since Transformers lack inherent sequential structure.\n",
    "- Transformer Encoder: Stacks multiple Transformer layers (multi-head attention, feed-forward layers, normalization) to process patch embeddings.\n",
    "- MLP Head: Maps the final output from the class token to the classes for prediction.\n",
    "\n",
    "#### Vision Transformer Terminology\n",
    "- Attention Mechanism: Allows the model to selectively focus on relevant parts of the sequence.\n",
    "- Multi-Head Self-Attention: Applies multiple attention heads, allowing the model to capture various aspects of patch relationships.\n",
    "- Position Embedding: Adds positional information to patches so the model can understand their relative positions.\n",
    "- Feed-Forward Network: Processes each patch independently after the attention layer in each Transformer block.\n",
    "\n",
    "#### Vision Transformer Code Implementation\n",
    "Here's a code implementation of the Vision Transformer with comments explaining the flow and importance of each part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Vision Transformer (ViT) model class\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000, \n",
    "                 d_model=768, num_heads=12, num_layers=12, mlp_dim=3072, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate the number of patches by dividing the image dimensions by patch size.\n",
    "        # For example, a 224x224 image with a 16x16 patch size results in 196 patches (14x14).\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Calculate the dimensionality of each patch, which is flattened to form a vector.\n",
    "        # For RGB images, each patch has `in_channels * patch_size^2` values.\n",
    "        self.patch_dim = in_channels * patch_size * patch_size\n",
    "        \n",
    "        # Linear layer to project each flattened patch to a feature vector of dimension `d_model`.\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, d_model)\n",
    "        \n",
    "        # A learnable class token, which is prepended to the sequence of patch embeddings.\n",
    "        # This class token will hold information for image classification after training.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        \n",
    "        # Positional encoding for each patch and the class token.\n",
    "        # This helps the model understand the relative positions of patches.\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, self.num_patches + 1, d_model))\n",
    "        \n",
    "        # Stack of Transformer encoder layers (number of layers specified by `num_layers`).\n",
    "        # Each layer is a Transformer block that processes the sequence of patch embeddings.\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final classification head which maps the class token's final embedding to output classes.\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),  # Normalizes the class token's embedding before classification.\n",
    "            nn.Linear(d_model, num_classes)  # Maps the embedding to the output class space.\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convert the input image batch into a sequence of flattened patches.\n",
    "        x = self.to_patches(x)\n",
    "        \n",
    "        # Project each patch embedding to the desired model dimension (`d_model`).\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        # Prepare the class token and expand it to match the batch size.\n",
    "        # Concatenate it at the beginning of each sequence of patch embeddings.\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add positional encodings to the patch embeddings, including the class token.\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        # Pass the sequence of embeddings through each Transformer layer.\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Extract the class token's output after the Transformer layers\n",
    "        # and pass it through the classification head.\n",
    "        return self.mlp_head(x[:, 0])\n",
    "\n",
    "    def to_patches(self, x):\n",
    "        # Divide the input image into non-overlapping patches.\n",
    "        patch_size = int(self.patch_dim ** 0.5)  # Calculate patch size (e.g., 16x16).\n",
    "        x = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)  # Extract patches.\n",
    "        \n",
    "        # Rearrange the patches to form a sequence and flatten each patch to a vector.\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(x.size(0), -1, self.patch_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Transformer Encoder Layer class used in each layer of the Vision Transformer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mlp_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention layer to capture relationships between patches.\n",
    "        # Each head can focus on different parts of the sequence.\n",
    "        self.multi_head_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout_rate)\n",
    "        \n",
    "        # Layer normalization applied before the attention mechanism.\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network (MLP) for additional non-linearity, applied independently to each patch.\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, mlp_dim),  # Expands the embedding dimension to `mlp_dim`.\n",
    "            nn.ReLU(),                    # Applies ReLU activation.\n",
    "            nn.Dropout(dropout_rate),     # Applies dropout for regularization.\n",
    "            nn.Linear(mlp_dim, d_model),  # Reduces the dimension back to `d_model`.\n",
    "            nn.Dropout(dropout_rate)      # Applies dropout again for regularization.\n",
    "        )\n",
    "        \n",
    "        # Second layer normalization applied after the feed-forward network.\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply multi-head attention to the input sequence with a residual connection.\n",
    "        # The residual connection helps in stabilizing training by preserving information.\n",
    "        x = x + self.multi_head_attn(x, x, x)[0]\n",
    "        x = self.norm1(x)  # Apply layer normalization.\n",
    "        \n",
    "        # Apply the feed-forward network with another residual connection.\n",
    "        x = x + self.ff(x)\n",
    "        return self.norm2(x)  # Final layer normalization before outputting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Code Flow and Key Components\n",
    "- Image to Patch Conversion (to_patches):\n",
    "\n",
    "    - The image is divided into non-overlapping patches. Each patch is flattened and reshaped to match the patch dimension, allowing it to be used as a token.\n",
    "- Patch Embedding:\n",
    "\n",
    "    - Each flattened patch is linearly projected to the model’s dimension (d_model). This step converts spatial information into a token vector.\n",
    "- Class Token:\n",
    "\n",
    "    - A learnable vector (cls_token) is added to represent the image at a global level. This token’s final state after Transformer layers is used for classification.\n",
    "- Positional Encoding:\n",
    "\n",
    "    - Adds positional information to patches, essential for spatial understanding. Without this, patches would lose their spatial relations.\n",
    "- Transformer Encoder Layers:\n",
    "\n",
    "    - Each layer applies multi-head self-attention and feed-forward operations, followed by layer normalization. The attention mechanism helps the model focus on relevant patches.\n",
    "- Classification Head:\n",
    "\n",
    "    - Uses the final output of the class token to predict the image class.\n",
    "\n",
    "#### Improvements Since 2017\n",
    "Since the Transformer was introduced in NLP, significant adaptations have been made to apply it effectively to images:\n",
    "\n",
    "- Vision Transformers (ViT): Adapted to use patches instead of individual pixels, making Transformers feasible for large images.\n",
    "- Data-Efficient Training: Data augmentation techniques (e.g., DeiT) improved performance in image classification without needing massive datasets.\n",
    "- Hybrid Architectures: Models like Swin Transformer added inductive biases (e.g., local attention) that help the model understand local structures better.\n",
    "\n",
    "#### Potential Future Improvements\n",
    "- Improved Positional Encoding: Dynamic or learned positional encodings can adapt better to image structures and larger resolutions.\n",
    "- Efficient Attention Mechanisms: Methods like sparse or low-rank approximations could reduce computational load in handling high-resolution images.\n",
    "- Hybrid CNN-Transformer Models: Combining CNN’s local pattern recognition with Transformer’s global attention may improve model robustness and generalization.\n",
    "- Enhanced Patch Embedding: Using richer representations for patches (e.g., multi-layer CNN features) could provide better initialization and improve learning efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
