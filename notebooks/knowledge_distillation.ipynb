{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge distillation is a process in machine learning where a smaller, simpler model (student) is trained to replicate the behavior of a larger, more complex model (teacher). The teacher model, usually a highly accurate but resource-intensive neural network, generates \"soft labels\" (probabilistic outputs) for the training data. The student model then learns from these outputs, capturing the knowledge of the teacher in a compressed form.\n",
    "\n",
    "This approach helps make models more efficient, reducing memory and computational requirements while maintaining accuracy levels close to the original large model. Knowledge distillation is particularly useful for deploying machine learning models on resource-constrained devices, like mobile phones.\n",
    "\n",
    "Let's go through a simple use case of knowledge distillation using Python and PyTorch. In this example, we’ll use the MNIST dataset to distill knowledge from a large \"teacher\" model to a smaller \"student\" model.\n",
    "\n",
    "- Use Case\n",
    "Imagine we have a large teacher model (like a deep neural network) that performs well on handwritten digit classification (MNIST dataset). However, deploying this model on mobile devices is impractical due to its size. Knowledge distillation allows us to create a smaller student model that learns to mimic the teacher's performance while being lightweight enough for mobile deployment.\n",
    "\n",
    "- Steps\n",
    "    - Train the teacher model on the MNIST dataset.\n",
    "    - Use the teacher model to generate \"soft labels\" (probabilistic outputs) for the dataset.\n",
    "    - Train the student model using these soft labels.\n",
    "\n",
    "- Code Example\n",
    "    - Step 1: Set Up and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <EEB3232B-F6A7-3262-948C-BB2F54905803> /Users/rshankar/anaconda3/envs/cv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 11504468.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 311977.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1307418.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 1347374.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define transformations and load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 2: Define Teacher and Student Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple teacher model\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define a smaller student model\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 3: Train the Teacher Model (standard training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = TeacherModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the teacher model (simplified for demonstration purposes)\n",
    "teacher_model.train()\n",
    "for epoch in range(5):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = teacher_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 4: Distill Knowledge to the Student Model\n",
    "\n",
    "Knowledge distillation introduces a \"temperature\" parameter to soften the teacher’s output probabilities, which makes learning from them easier for the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_logits, teacher_logits, temperature):\n",
    "    distillation_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "    student_probs = nn.functional.log_softmax(student_logits / temperature, dim=1)\n",
    "    teacher_probs = nn.functional.softmax(teacher_logits / temperature, dim=1)\n",
    "    return distillation_loss_fn(student_probs, teacher_probs)\n",
    "\n",
    "# Initialize student model\n",
    "student_model = StudentModel()\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
    "temperature = 5.0  # Adjust temperature for distillation\n",
    "\n",
    "# Train the student model with distillation loss\n",
    "student_model.train()\n",
    "for epoch in range(5):\n",
    "    for images, labels in train_loader:\n",
    "        # Get teacher predictions\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(images)\n",
    "        \n",
    "        # Get student predictions\n",
    "        student_outputs = student_model(images)\n",
    "        \n",
    "        # Compute distillation loss\n",
    "        loss = distillation_loss(student_outputs, teacher_outputs, temperature)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation\n",
    "    - Teacher Model Training: The teacher model is trained on the MNIST data with standard cross-entropy loss.\n",
    "    - Distillation Loss: The student model is trained using a KL Divergence loss between its output probabilities and the softened probabilities from the teacher model.\n",
    "    - Temperature: By setting a temperature (e.g., temperature=5.0), we control how \"soft\" the teacher's predictions are, allowing the student model to learn better from less confident outputs.\n",
    "\n",
    "- Benefits\n",
    "\n",
    "After training, the student model is much smaller than the teacher but retains a similar level of performance. This approach saves memory and computation, making the student model suitable for deployment on resource-constrained devices.\n",
    "\n",
    "This code example shows the essential steps in knowledge distillation, providing a lightweight model with reasonable accuracy.\n",
    "\n",
    "##### Knowledge Distillation: Applications and Limitations\n",
    "Knowledge distillation can be applied to a wide range of machine learning applications, especially where there's a need to deploy efficient, smaller models that retain most of the accuracy of a larger model. However, its effectiveness varies by task and model architecture. Here’s when it’s most useful and when it may have limitations:\n",
    "\n",
    "##### Where It’s Effective\n",
    "- **Image Classification and NLP**: Distillation has been widely used in tasks like image classification, text classification, and language translation, where the student model can benefit significantly from the knowledge of a large pre-trained teacher model.\n",
    "- **Resource-Constrained Deployments**: Distillation is valuable in scenarios like mobile apps, IoT devices, and edge computing, where memory, storage, or processing power is limited.\n",
    "- **Real-Time Applications**: Small, distilled models are ideal for applications requiring real-time performance, as they reduce inference time while maintaining accuracy.\n",
    "\n",
    "##### Limitations\n",
    "- **Complex Tasks with Unique Outputs**: In cases like image generation or certain types of reinforcement learning, where outputs are not straightforward classifications, distillation might be less effective.\n",
    "- **Task-Specific Data Requirements**: For some tasks, student models might not capture subtle nuances if the teacher's knowledge doesn’t translate well into simplified representations, especially if the student model is much smaller.\n",
    "- **Architectural Constraints**: Distillation is typically more effective when the teacher and student architectures are similar. If they differ significantly, the student model might struggle to approximate the teacher's behavior.\n",
    "\n",
    "##### Summary\n",
    "Knowledge distillation is a powerful tool for model compression and deployment on limited-resource devices, but it may not be universally optimal, especially for highly complex or generative tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
