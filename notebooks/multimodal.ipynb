{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal Models\n",
    "\n",
    "A multimodal transformer is designed to process and integrate information from multiple data modalities, such as text and images. This architecture combines the embeddings of both modalities into a joint representation, allowing the model to capture complex interactions and correlations between different types of data. For example, a multimodal transformer can take an image and associated text (e.g., a caption) as input and generate descriptive or interpretive text as output.\n",
    "\n",
    "#### Code Architecture: Multimodal Transformer with Text and Image Inputs, Text Output\n",
    "This architecture includes:\n",
    "\n",
    "- Text Embedding: Embeds text tokens into a suitable format.\n",
    "- Image Embedding: Divides the image into patches and embeds each patch as a token.\n",
    "- Positional Encoding: Adds positional information to both text and image embeddings.\n",
    "- Fusion Layer: Merges the text and image embeddings into a unified representation.\n",
    "- Transformer Layers: Processes the combined embeddings using self-attention to extract high-level features.\n",
    "- Decoder for Text Generation: Uses the combined representation to generate a sequence of output text tokens.\n",
    "\n",
    "Hereâ€™s the multimodal transformer code architecture with detailed comments:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Multimodal Transformer model that combines text and image inputs, and outputs text\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, img_size=224, patch_size=16, in_channels=3, d_model=768,\n",
    "                 num_heads=12, num_layers=6, mlp_dim=3072, dropout_rate=0.1, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text Embedding layer:\n",
    "        # Maps each text token to a fixed-size vector of 'd_model' dimensions.\n",
    "        # 'vocab_size' specifies the number of unique tokens, typically the vocabulary size.\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Image patch embedding setup:\n",
    "        # Calculates the number of patches (non-overlapping small image sections).\n",
    "        # Each patch has dimensions of 'patch_dim' and is flattened and mapped to 'd_model' dimensions.\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = in_channels * patch_size * patch_size\n",
    "        self.image_patch_embedding = nn.Linear(self.patch_dim, d_model)\n",
    "        \n",
    "        # Class token and positional encodings:\n",
    "        # A special learnable 'cls_token' represents a global summary of the image content.\n",
    "        # Positional encodings add sequence information to text and image embeddings.\n",
    "        # Text encoding assumes a max length of 512 tokens.\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.text_pos_encoding = nn.Parameter(torch.zeros(1, 512, d_model))  # Positional encoding for text\n",
    "        self.image_pos_encoding = nn.Parameter(torch.zeros(1, self.num_patches + 1, d_model))  # For image patches\n",
    "        \n",
    "        # Transformer layers:\n",
    "        # Stack of Transformer encoder layers that learn relationships within and between modalities.\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Text generation head:\n",
    "        # A final layer to map the output representation to the vocabulary space for generating words.\n",
    "        self.text_generation_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_tokens, images):\n",
    "        # Text token embedding\n",
    "        text_embed = self.text_embedding(text_tokens)  # Embeds each text token into a 'd_model'-dim vector\n",
    "        \n",
    "        # Adding text positional encoding to text embeddings to preserve word order information\n",
    "        text_embed = text_embed + self.text_pos_encoding[:, :text_embed.size(1), :]\n",
    "        \n",
    "        # Image processing:\n",
    "        # Converts images into non-overlapping patches, flattens them, and embeds each patch to 'd_model' dimensions.\n",
    "        image_embed = self.to_image_patches(images)\n",
    "        image_embed = self.image_patch_embedding(image_embed)\n",
    "        \n",
    "        # Class token:\n",
    "        # Adds a special token to the beginning of each image embedding sequence, followed by positional encodings.\n",
    "        batch_size = image_embed.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        image_embed = torch.cat((cls_tokens, image_embed), dim=1)\n",
    "        image_embed = image_embed + self.image_pos_encoding[:, :image_embed.size(1), :]\n",
    "        \n",
    "        # Concatenating text and image embeddings for multimodal representation learning\n",
    "        multimodal_embed = torch.cat((text_embed, image_embed), dim=1)\n",
    "        \n",
    "        # Sequentially passing through transformer layers to capture multimodal relationships\n",
    "        for layer in self.transformer_layers:\n",
    "            multimodal_embed = layer(multimodal_embed)\n",
    "        \n",
    "        # Output layer for text generation:\n",
    "        # Uses the 'cls_token' embedding as input to the text generation head to predict the next text token.\n",
    "        text_output = self.text_generation_head(multimodal_embed[:, 0])  # Shape: [batch_size, vocab_size]\n",
    "        return text_output\n",
    "\n",
    "    def to_image_patches(self, images):\n",
    "        # Split image into non-overlapping patches:\n",
    "        # This function reshapes the image into small, flattened patches for processing as sequential input.\n",
    "        patch_size = int(self.patch_dim ** 0.5)  # Calculate patch size (e.g., 16x16)\n",
    "        \n",
    "        # Unfolding image into patches and reshaping into the desired format\n",
    "        images = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "        \n",
    "        # Flattening patches and reformatting to a sequence of patches with 'patch_dim' dimensions\n",
    "        images = images.permute(0, 2, 3, 1, 4, 5).contiguous().view(images.size(0), -1, self.patch_dim)\n",
    "        return images\n",
    "\n",
    "\n",
    "# Transformer Encoder Layer definition\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mlp_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention layer:\n",
    "        # Learns relationships among tokens, allowing the model to attend to different parts of the input sequence.\n",
    "        self.multi_head_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout_rate)\n",
    "        \n",
    "        # Layer Normalization for stabilizing training and improving convergence\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward layer:\n",
    "        # Applies non-linearity and projects token representations to higher dimensions, learning complex features.\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_dim, d_model),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Second Layer Normalization applied after the feed-forward network\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Multi-head attention:\n",
    "        # The token sequence interacts with itself to learn dependencies between tokens.\n",
    "        x = x + self.multi_head_attn(x, x, x)[0]\n",
    "        \n",
    "        # First normalization and residual connection:\n",
    "        # Helps stabilize and optimize learning by preserving important representations.\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward transformation with residual connection:\n",
    "        # Adds further transformation to token representations, capturing non-linear dependencies.\n",
    "        x = x + self.ff(x)\n",
    "        \n",
    "        # Second normalization before output:\n",
    "        # Returns processed sequence for subsequent layers\n",
    "        return self.norm2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Explanation and Flow\n",
    "- Text Embedding:\n",
    "\n",
    "    - self.text_embedding: Converts text tokens to embeddings of d_model dimensions.\n",
    "    - text_pos_encoding: Positional encodings for text are added to capture token positions within the sequence.\n",
    "- Image Embedding:\n",
    "\n",
    "    - self.image_patch_embedding: Divides the image into patches, flattens each patch, and projects it to d_model dimensions.\n",
    "- Class Token and Positional Encoding:\n",
    "\n",
    "    - self.cls_token: A learnable class token that will capture global multimodal representation.\n",
    "    - text_pos_encoding and image_pos_encoding: Positional encodings for text and image patches, which preserve order within each modality.\n",
    "\n",
    "- Transformer Layers:\n",
    "\n",
    "    - self.transformer_layers: Each TransformerEncoderLayer applies multi-head attention and feed-forward networks with residual connections for the entire concatenated sequence of text and image embeddings.\n",
    "- Text Generation Head:\n",
    "\n",
    "    - self.text_generation_head: Final classification layer that maps the output to vocabulary size for text generation.\n",
    "\n",
    "#### Improvements Since 2017\n",
    "Since 2017, multimodal transformer architectures have seen several enhancements:\n",
    "\n",
    "- Patch-based Embedding for Images: Inspired by the Vision Transformer, image data is converted to patches, allowing more structured processing and efficient self-attention computation.\n",
    "- Joint Attention Mechanisms: Cross-attention and joint-attention layers allow the model to focus on relevant areas across modalities.\n",
    "- Positional Encoding for Multimodal Input: Positional encodings are more sophisticated, allowing each modality to have distinct embeddings that can be blended effectively.\n",
    "- Efficient Transformers: Techniques like sparse attention, Linformer, and Performer reduce the computational overhead, making multimodal transformers feasible for larger datasets.\n",
    "\n",
    "#### Potential Future Improvements\n",
    "- Modality-specific Attention Mechanisms: Introducing custom attention heads optimized for each modality.\n",
    "- Dynamic Fusion Strategies: Instead of fixed concatenation, dynamic fusion techniques could adjust based on data context.\n",
    "- Memory Efficiency: Further memory optimization is essential to support high-resolution images or longer text sequences.\n",
    "- Pre-trained Multimodal Models: Pre-training on massive multimodal datasets can improve the generalization capabilities across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
