{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A knowledge graph represents semantic relationships between entities, with nodes representing entities like objects, people, or places, and edges defining their relationships, such as \"Ottawa\" being the \"capital\" of \"Canada.\" These graphs are used by virtual assistants to answer queries and can model diverse relationships, such as movies and actors or recipes and ingredients. By integrating data from multiple sources, like census data and online reviews, knowledge graphs infer facts more accurately, such as estimating the number of Chinese restaurants in New York. Built using Natural Language Processing (NLP) through semantic enrichment, they transform unstructured text into structured knowledge. Commercially, knowledge graphs power recommendation systems (e.g., YouTube videos or retail product pairings), detect fraudulent insurance claims, and improve product recommendations. The transcript concludes with a simple, humorous graph illustrating \"human,\" \"coffee,\" and \"sleep,\" where \"human consumes coffee,\" \"human needs sleep,\" and \"coffee prevents sleep,\" advising against caffeine after 5 PM.\n",
    "\n",
    "Building a knowledge graph for an e-commerce website can significantly enhance both search and recommendation systems by organizing product data into a structured format. Let's break it down step by step.\n",
    "\n",
    "Step 1: Define the Objective and Dataset\n",
    "Objective: The goal is to create a product knowledge graph to improve search and recommendation on an e-commerce website. The knowledge graph will capture product relationships, attributes, and categories, allowing the website to:\n",
    "\n",
    "- Provide more relevant search results.\n",
    "- Make personalized product recommendations.\n",
    "- Better understand product attributes for better categorization and user experience.\n",
    "\n",
    "Dataset: For the demonstration, we need a real-world e-commerce dataset that is small, easily loaded, and suitable for building a knowledge graph. A common open-source dataset to use is Amazon product data, specifically product metadata (e.g., title, description, category, and price). A sample dataset can be obtained from: https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>bought_together</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Digital Music</td>\n",
       "      <td>Baja Marimba Band</td>\n",
       "      <td>4.9</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'Date First Available': 'February 28, 2010'}</td>\n",
       "      <td>B000V87RP2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Music</td>\n",
       "      <td>'80s Halloween-All Original Artists &amp; Recordings</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>14.98</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Love and Rockets  (Artist),     Duran Duran  (...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'Package Dimensions': '5.55 x 4.97 x 0.54 inc...</td>\n",
       "      <td>B0062F0MJQ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Digital Music</td>\n",
       "      <td>TRIO +1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['CD ALBUM']</td>\n",
       "      <td>57.99</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Rob Wasserman   Format: Audio CD</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'Is Discontinued By Manufacturer': 'No', 'Pac...</td>\n",
       "      <td>B00005GT12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Digital Music</td>\n",
       "      <td>Gold and Silver: Lehar, Delibes, Lanner, Johan...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>29.91</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Franz Lehar  (Composer),     Leo Delibes  (Com...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'Manufacturer': 'Hungaroton / White Label', '...</td>\n",
       "      <td>B0007PD2BW</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Digital Music</td>\n",
       "      <td>Grateful Dead Dave's Picks Volume 25 Live at B...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Sold out. Numbered limited edition']</td>\n",
       "      <td>149.99</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Grateful Dead  (Artist, Orchestra)    Format: ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'Package Dimensions': '5.55 x 4.97 x 0.54 inc...</td>\n",
       "      <td>B079CPD45R</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   main_category                                              title  \\\n",
       "0  Digital Music                                  Baja Marimba Band   \n",
       "1  Digital Music   '80s Halloween-All Original Artists & Recordings   \n",
       "2  Digital Music                                            TRIO +1   \n",
       "3  Digital Music  Gold and Silver: Lehar, Delibes, Lanner, Johan...   \n",
       "4  Digital Music  Grateful Dead Dave's Picks Volume 25 Live at B...   \n",
       "\n",
       "   average_rating  rating_number features  \\\n",
       "0             4.9              8       []   \n",
       "1             5.0              3       []   \n",
       "2             5.0              1       []   \n",
       "3             5.0              1       []   \n",
       "4             4.9             20       []   \n",
       "\n",
       "                              description   price  \\\n",
       "0                                      []     NaN   \n",
       "1                                      []   14.98   \n",
       "2                            ['CD ALBUM']   57.99   \n",
       "3                                      []   29.91   \n",
       "4  ['Sold out. Numbered limited edition']  149.99   \n",
       "\n",
       "                                              images videos  \\\n",
       "0  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
       "1  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
       "2  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
       "3  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
       "4  [{'thumb': 'https://m.media-amazon.com/images/...     []   \n",
       "\n",
       "                                               store categories  \\\n",
       "0                                                NaN         []   \n",
       "1  Love and Rockets  (Artist),     Duran Duran  (...         []   \n",
       "2                   Rob Wasserman   Format: Audio CD         []   \n",
       "3  Franz Lehar  (Composer),     Leo Delibes  (Com...         []   \n",
       "4  Grateful Dead  (Artist, Orchestra)    Format: ...         []   \n",
       "\n",
       "                                             details parent_asin  \\\n",
       "0      {'Date First Available': 'February 28, 2010'}  B000V87RP2   \n",
       "1  {'Package Dimensions': '5.55 x 4.97 x 0.54 inc...  B0062F0MJQ   \n",
       "2  {'Is Discontinued By Manufacturer': 'No', 'Pac...  B00005GT12   \n",
       "3  {'Manufacturer': 'Hungaroton / White Label', '...  B0007PD2BW   \n",
       "4  {'Package Dimensions': '5.55 x 4.97 x 0.54 inc...  B079CPD45R   \n",
       "\n",
       "   bought_together  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset (this can be a CSV file with product metadata)\n",
    "df = pd.read_csv('/Users/rshankar/Downloads/Projects/deep-learning/amazon-search-recommend/data/Digital_Music_Meta.csv')\n",
    "\n",
    "# Preview the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70537, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Refine, Clean, and Extract Attributes Using NLP\n",
    "To build a knowledge graph, we need to refine and extract relevant attributes (e.g., product features, brand, material, etc.). This can be achieved using NLP techniques such as Named Entity Recognition (NER) or an OpenAI LLM to extract structured attributes.\n",
    "\n",
    "For the cleaning process, we can use text pre-processing techniques to remove unnecessary words, correct misspellings, and extract key attributes. Then we use NLP or an LLM model to extract structured entities like \"Brand\", \"Material\", \"Color\", etc., from the product descriptions.\n",
    "\n",
    "Hereâ€™s how we can clean and refine the attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>extracted_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baja Marimba Band</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'80s Halloween-All Original Artists &amp; Recordings</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRIO +1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gold and Silver: Lehar, Delibes, Lanner, Johan...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grateful Dead Dave's Picks Volume 25 Live at B...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title extracted_entities\n",
       "0                                  Baja Marimba Band                 []\n",
       "1   '80s Halloween-All Original Artists & Recordings                 []\n",
       "2                                            TRIO +1                 []\n",
       "3  Gold and Silver: Lehar, Delibes, Lanner, Johan...                 []\n",
       "4  Grateful Dead Dave's Picks Volume 25 Live at B...                 []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load SpaCy model for NER (if not installed, use pip install spacy)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove special characters\n",
    "    return text.strip().lower()\n",
    "\n",
    "# Example: Use SpaCy for NER to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Refine attributes for the dataset\n",
    "df['cleaned_description'] = df['description'].apply(clean_text)\n",
    "df['extracted_entities'] = df['cleaned_description'].apply(extract_entities)\n",
    "\n",
    "# Preview the cleaned and extracted entities\n",
    "df[['title', 'extracted_entities']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>extracted_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35894</th>\n",
       "      <td>Omen Escape to Nowhere</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50831</th>\n",
       "      <td>Britten/Berkeley: Complete Works for Voice and...</td>\n",
       "      <td>[(16, CARDINAL), (chinese, NORP), (58713, DATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41734</th>\n",
       "      <td>Berg;Chamber Concerto</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8031</th>\n",
       "      <td>Tribal Eyes</td>\n",
       "      <td>[(1, CARDINAL), (2, CARDINAL), (3, CARDINAL), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39365</th>\n",
       "      <td>I Can Hear It Now... 1919-1932, Vol. 3</td>\n",
       "      <td>[(record vinyl, PERSON)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "35894                             Omen Escape to Nowhere   \n",
       "50831  Britten/Berkeley: Complete Works for Voice and...   \n",
       "41734                              Berg;Chamber Concerto   \n",
       "8031                                         Tribal Eyes   \n",
       "39365             I Can Hear It Now... 1919-1932, Vol. 3   \n",
       "\n",
       "                                      extracted_entities  \n",
       "35894                                                 []  \n",
       "50831  [(16, CARDINAL), (chinese, NORP), (58713, DATE...  \n",
       "41734                                                 []  \n",
       "8031   [(1, CARDINAL), (2, CARDINAL), (3, CARDINAL), ...  \n",
       "39365                           [(record vinyl, PERSON)]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title', 'extracted_entities']].sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize LM Studio client\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Function to extract attributes using LM Studio\n",
    "def extract_attributes_with_lm_studio(description):\n",
    "    # Make an API call to the LLM\n",
    "    response = requests.post(\n",
    "        \"http://localhost:1234/v1/chat/completions\",\n",
    "        json={\n",
    "            \"model\": \"lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Extract product attributes from the description.\"},\n",
    "                {\"role\": \"user\", \"content\": description}\n",
    "            ],\n",
    "            \"temperature\": 0.7,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('choices')[0]['message']['content']\n",
    "    else:\n",
    "        return None  # Handle API errors or missing content\n",
    "\n",
    "# Initialize tqdm to show progress bar with apply\n",
    "def extract_with_progress(row):\n",
    "    # row is the entire DataFrame row, access description using row['description']\n",
    "    extracted_attribute = extract_attributes_with_lm_studio(row['description'])\n",
    "    return extracted_attribute\n",
    "\n",
    "# Apply the function to extract attributes with tqdm for progress bar\n",
    "df['extracted_attributes1'] = list(tqdm(df.apply(extract_with_progress, axis=1), total=len(df), desc=\"Extracting Attributes\"))\n",
    "\n",
    "# Show the results\n",
    "print(df[['title', 'extracted_attributes1']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build the Product Knowledge Graph\n",
    "A knowledge graph consists of nodes (entities like products, categories, brands) and edges (relationships between them). After extracting key product attributes, we can use these attributes to form the nodes and relationships.\n",
    "\n",
    "For example:\n",
    "\n",
    "Nodes: Product ID, Brand, Category, Material\n",
    "Edges: Product -> Belongs to -> Category, Product -> Made of -> Material, Product -> Manufactured by -> Brand\n",
    "We can use libraries like networkx to build the knowledge graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "KG = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "for _, row in df.iterrows():\n",
    "    product_id = row['parent_asin']\n",
    "    brand = row['extracted_attributes'].get('Brand', 'Unknown')\n",
    "    category = row['store']\n",
    "    \n",
    "    # Add product node\n",
    "    KG.add_node(product_id, type='product', name=row['title'])\n",
    "    \n",
    "    # Add brand and category nodes\n",
    "    KG.add_node(brand, type='brand')\n",
    "    KG.add_node(category, type='category')\n",
    "\n",
    "    # Add edges\n",
    "    KG.add_edge(product_id, brand, relationship='Manufactured by')\n",
    "    KG.add_edge(product_id, category, relationship='Belongs to')\n",
    "\n",
    "# Visualize the knowledge graph (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(KG, with_labels=True, node_size=3000, node_color='skyblue', font_size=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
