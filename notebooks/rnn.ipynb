{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RNN (Recurrent Neural Network)?\n",
    "Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to recognize patterns in sequences of data, such as time series or natural language. They are particularly useful for tasks where the context of the input data is crucial because they maintain a hidden state that captures information about previous inputs in the sequence.\n",
    "\n",
    "#### Use Cases for RNNs\n",
    "- Natural Language Processing (NLP):\n",
    "\n",
    "    - Sentiment analysis\n",
    "    - Language translation\n",
    "    - Text generation\n",
    "- Time Series Prediction:\n",
    "\n",
    "    - Stock price prediction\n",
    "    - Weather forecasting\n",
    "    - Speech Recognition:\n",
    "\n",
    "- Converting spoken language into text\n",
    "    - Music Generation:\n",
    "\n",
    "- Composing music based on learned patterns\n",
    "    - Image Captioning:\n",
    "\n",
    "- Generating textual descriptions of images\n",
    "\n",
    "#### Generate Random Data for RNN\n",
    "We'll generate some random sequential data suitable for training an RNN. For simplicity, let's create a dataset of sequences of numbers, where the goal is to predict the next number in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data shape: (1000, 10, 1)\n",
      "Generated target shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to generate sequential data\n",
    "def generate_sequence_data(num_sequences=1000, sequence_length=10):\n",
    "    # Generate random sequences of integers\n",
    "    X = np.random.randint(0, 100, (num_sequences, sequence_length))\n",
    "    # The target is the next number in the sequence\n",
    "    y = np.array([sequence[-1] for sequence in X])\n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_sequence_data()\n",
    "\n",
    "# Reshape X for RNN input (num_samples, time_steps, features)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Generated data shape:\", X.shape)\n",
    "print(\"Generated target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement RNN from Scratch\n",
    "Hereâ€™s a simple implementation of an RNN from scratch using NumPy. This will include the basic forward pass and loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: [[0.00190613]]\n",
      "Loss: 5475.717895768589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize parameters\n",
    "        self.input_size = input_size  # Number of input features\n",
    "        self.hidden_size = hidden_size  # Number of hidden units\n",
    "        self.output_size = output_size  # Number of output features\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01  # Input to hidden\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
    "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01  # Hidden to output\n",
    "        \n",
    "        # Bias vectors\n",
    "        self.b_h = np.zeros((1, hidden_size))  # Hidden bias\n",
    "        self.b_y = np.zeros((1, output_size))  # Output bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the RNN.\"\"\"\n",
    "        h = np.zeros((1, self.hidden_size))  # Initial hidden state\n",
    "        for t in range(x.shape[1]):\n",
    "            h = np.tanh(np.dot(x[:, t, :], self.W_xh) + np.dot(h, self.W_hh) + self.b_h)\n",
    "        y = np.dot(h, self.W_hy) + self.b_y\n",
    "        return y\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Compute the loss using Mean Squared Error.\"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1   # We have a single feature (the number itself)\n",
    "hidden_size = 5  # Number of hidden units\n",
    "output_size = 1  # Predicting the next number\n",
    "\n",
    "# Create RNN model\n",
    "rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass example\n",
    "y_pred = rnn.forward(X[0:1])  # Forward pass for the first sequence\n",
    "loss = rnn.compute_loss(y_pred, np.array([[y[0]]]))  # Calculate loss\n",
    "print(\"Predicted output:\", y_pred)\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use RNNs\n",
    "- Use RNNs When:\n",
    "\n",
    "    - You have sequential data where the order of inputs matters (e.g., time series, text).\n",
    "    - The context or previous information is crucial for prediction (e.g., in language processing).\n",
    "- Do Not Use RNNs When:\n",
    "\n",
    "    - The data is independent and does not have a temporal or sequential structure.\n",
    "    - You have long sequences where standard RNNs struggle due to vanishing gradients (consider using LSTM or GRU instead).\n",
    "\n",
    "#### Loss Function\n",
    "The typical loss function used for training RNNs, especially for regression tasks, is Mean Squared Error (MSE), while for classification tasks, Cross-Entropy Loss is often used.\n",
    "\n",
    "#### Optimizing the Algorithm\n",
    "To optimize the RNN algorithm:\n",
    "\n",
    "- Gradient Descent: Use algorithms like Adam or RMSProp for effective optimization.\n",
    "- Batch Training: Instead of training on one sequence at a time, use mini-batches to stabilize training.\n",
    "- Regularization: Implement dropout or L2 regularization to prevent overfitting.\n",
    "- Use LSTM/GRU: For long sequences, consider using Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) architectures to mitigate vanishing gradient problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
