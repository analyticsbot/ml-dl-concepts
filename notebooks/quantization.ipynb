{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What Does Quantization Do?\n",
    "Quantization maps floating-point numbers (`float32`) to lower-precision integers (`int8`). This is one of the possibilities and several other exist. In this case, this involves:\n",
    "- **Scaling:** Mapping the range of `float32` values to the range of `int8` (-128 to 127).\n",
    "- **Rounding:** Converting decimal values to the nearest integer.\n",
    "\n",
    "#### 2. Why Does This Make Sense?\n",
    "- **Efficiency:** `float32` weights take 4 bytes per weight, while `int8` weights take 1 byte. This reduces model size and improves performance, especially on resource-constrained devices.\n",
    "| **Number of Weights** | **Model Size (float32)** | **Model Size (int8)** | **Space Saved (in %)** |\n",
    "|-----------------------|--------------------------|-----------------------|------------------------|\n",
    "| 1 Million (1M)        | 4MB (4 bytes * 1M)       | 1MB (1 byte * 1M)      | 75%                    |\n",
    "| 1 Billion (1B)        | 4GB (4 bytes * 1B)       | 1GB (1 byte * 1B)      | 75%                    |\n",
    "| 70 Billion (70B)      | 280GB (4 bytes * 70B)    | 70GB (1 byte * 70B)    | 75%                    |\n",
    "\n",
    "By converting from `float32` to `int8`, the memory and disk space requirements are reduced by approximately 75%, as each weight is reduced from 4 bytes to 1 byte.\n",
    "\n",
    "- **Performance:** The loss of precision often has minimal impact on the model's accuracy. More on this is discuss below.\n",
    "\n",
    "#### 3. Quantization Process Example\n",
    "#### Step-by-Step:\n",
    "1. Calculate a **scaling factor** to map the `float32` range to the `int8` range.\n",
    "2. Multiply each `float32` value by the scaling factor.\n",
    "3. Round the result to the nearest integer.\n",
    "\n",
    "#### Example:\n",
    "Let’s assume the scaling factor is `1000` (simplified for illustration).\n",
    "\n",
    "- `float32 weight = 0.00431718`\n",
    "  - Scale: `0.00431718 * 1000 = 4.31718`\n",
    "  - Round: `4`\n",
    "  - `int8 weight = 4`\n",
    "\n",
    "- `float32 weight = -0.03002159`\n",
    "  - Scale: `-0.03002159 * 1000 = -30.02159`\n",
    "  - Round: `-30`\n",
    "  - `int8 weight = -30`\n",
    "\n",
    "#### 4. Sample Conversion Breakdown\n",
    "Assume the model has the following `float32` weights:\n",
    "\n",
    "**Original Float32 Weights (First Dense Layer):**\n",
    "```python\n",
    "[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]\n",
    "```\n",
    "\n",
    "After quantization, with scaling and rounding (scale=1000 in this example):\n",
    "\n",
    "**Quantized Int8 Weights:**\n",
    "```python\n",
    "[[ 4, -30, -19, 34, 15 ]]\n",
    "```\n",
    "\n",
    "#### How Do We Decide the Scaling Factor?\n",
    "\n",
    "1. **Analyze the Range of Float32 Weights:**\n",
    "   - Determine the **minimum** and **maximum** values in the float32 weight matrix.\n",
    "   - Example: If the weights range from `-0.1` to `0.1`, the scaling factor needs to stretch this range to fit within the `int8` range `[-128, 127]`.\n",
    "\n",
    "2. **Compute the Scaling Factor:**\n",
    "   - A scaling factor `S` is calculated as:\n",
    "     \\[\n",
    "     S = \\frac{\\text{max}(\\text{abs}(\\text{weights}))}{127}\n",
    "     \\]\n",
    "   - This ensures the largest weight in the float32 matrix maps to `127`, the upper limit of `int8`.\n",
    "\n",
    "3. **Quantize the Weights:**\n",
    "   - Each weight is divided by `S` and rounded to the nearest integer.\n",
    "\n",
    "4. **Dequantize During Inference:**\n",
    "   - The int8 weights are scaled back to approximate float32 values during inference using the inverse of the scaling factor \\( S^{-1} \\).\n",
    "\n",
    "#### Limitations of This Approach\n",
    "\n",
    "1. **Loss of Precision:**\n",
    "   - Fine-grained differences in small weights might be lost when mapping to integers, potentially impacting performance in models highly sensitive to weight precision.\n",
    "\n",
    "2. **Dynamic Range:**\n",
    "   - Models with weights that span a wide dynamic range (e.g., very large and very small values) may suffer because a single scaling factor might not effectively represent all values.\n",
    "\n",
    "3. **Activation Quantization Challenges:**\n",
    "   - Activations (intermediate layer outputs) are also quantized, and their dynamic range can vary between inputs, leading to additional quantization errors.\n",
    "\n",
    "4. **Bias Quantization:**\n",
    "   - Bias terms in the model may require higher precision because small changes can have a significant impact. These are often kept in `float32` to mitigate accuracy loss.\n",
    "\n",
    "5. **Model-Specific Sensitivity:**\n",
    "   - Some models (e.g., those with small weights near zero or those with fine-grained layer dependencies) may degrade more in accuracy than others.\n",
    "\n",
    "6. **Hardware Dependence:**\n",
    "   - The performance gain depends on the hardware’s ability to optimize for int8 computations. Some devices may not fully support int8 arithmetic.\n",
    "\n",
    "#### How to Mitigate Limitations\n",
    "\n",
    "- **Use Per-Tensor Scaling:**\n",
    "  - Assign different scaling factors for each tensor or layer to better adapt to their specific ranges.\n",
    "  \n",
    "- **Use Quantization-Aware Training:**\n",
    "  - Simulate quantization effects during training to adjust the model’s weights for better post-quantization performance.\n",
    "\n",
    "- **Post-Training Calibration:**\n",
    "  - Run calibration with representative data to determine optimal scaling factors.\n",
    "\n",
    "- **Mixed-Precision Quantization:**\n",
    "  - Keep critical parts of the model (e.g., biases, certain layers) in higher precision (e.g., float16) while quantizing the rest to int8.\n",
    "\n",
    "By carefully choosing the scaling factor and mitigating limitations, quantization can achieve a good balance between model size, efficiency, and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Quantization in Machine Learning\n",
    "\n",
    "#### 1. Post-Training Static Quantization\n",
    "- **What it Does:** Converts weights and activations to lower precision (e.g., `float32` → `int8`) after training.\n",
    "- **How it Works:** Uses representative data to determine scaling factors for weights and activations.\n",
    "- **Space Savings:** ~75% (e.g., float32 → int8).\n",
    "- **Precision Loss:** Moderate; dependent on model structure and dynamic range.\n",
    "\n",
    "#### 2. Post-Training Dynamic Range Quantization\n",
    "- **What it Does:** Converts weights to int8 but leaves activations in float32 for dynamic scaling.\n",
    "\n",
    "    - **Weights**: These are the parameters of the model that are learned during training. They represent the strength of connections between neurons in different layers of the network. In neural networks, weights are multiplied with the input data to produce output.\n",
    "\n",
    "        **Example**: \n",
    "        - In a dense layer, weights could look like: `[ 0.23, -1.45, 0.78 ]`.\n",
    "\n",
    "    - **Activations**: These are the outputs of each layer (except the final output layer). After input data is processed through the layer's weights, the activation function is applied to the result. Activations represent the transformed information passed between layers.\n",
    "\n",
    "        **Example**: \n",
    "        - After applying weights to input data and the activation function, activations might look like: `[ 0.32, 0.47, 0.15 ]`.\n",
    "\n",
    "Let's consider a simple feedforward neural network used for classifying images (like MNIST digits).\n",
    "\n",
    "#### Model Architecture:\n",
    "- **Input Layer**: 28x28 pixel image (784 features)\n",
    "- **Hidden Layer 1**: 128 neurons\n",
    "- **Hidden Layer 2**: 64 neurons\n",
    "- **Output Layer**: 10 neurons (representing the 10 possible digit classes)\n",
    "\n",
    "#### Weights:\n",
    "- **What are they?**\n",
    "  Weights are the parameters of the network that are learned during training. In each layer, each connection between neurons has a weight that determines how much influence one neuron has on another.\n",
    "  \n",
    "- **In Our Example:**\n",
    "  - Between the input layer and hidden layer 1, each of the 784 input neurons has a weight for each of the 128 hidden layer neurons. This means there are `784 * 128 = 100,352` weights.\n",
    "  - Similarly, between hidden layer 1 and hidden layer 2, there are `128 * 64 = 8,192` weights.\n",
    "  - Between hidden layer 2 and the output layer, there are `64 * 10 = 640` weights.\n",
    "  \n",
    "  **Total number of weights in the model = 100,352 + 8,192 + 640 = 109,184 weights.**\n",
    "\n",
    "  **Example Weights:**\n",
    "  - Between input and first hidden layer: `[ 0.01, -0.23, 0.45, -0.56, ... ]`\n",
    "  - Between the first hidden layer and second hidden layer: `[ 0.32, -0.44, 0.13, ... ]`\n",
    "  - Between the second hidden layer and the output layer: `[ 0.03, 0.19, -0.56, ... ]`\n",
    "\n",
    "#### Activations:\n",
    "- **What are they?**\n",
    "  Activations are the outputs of each layer after applying the layer's weights and the activation function. They represent the information that gets passed from one layer to the next during the forward pass of the network.\n",
    "\n",
    "- **In Our Example:**\n",
    "  - The input image (28x28 pixels) is flattened into a 784-dimensional vector. This vector is multiplied by the weights of the input-hidden layer connections, and the result is passed through an activation function (e.g., ReLU).\n",
    "  - The activations for hidden layer 1 are the outputs of this activation function, which are passed to the next layer.\n",
    "  - The activations for the output layer are the final outputs of the network, which represent the predicted class probabilities.\n",
    "\n",
    "  **Example Activations:**\n",
    "  - After the input layer and hidden layer 1 computation, the activations might look like: `[ 0.65, 0.23, 0.01, ... ]`\n",
    "  - After hidden layer 2: `[ 0.89, 0.45, 0.74, ... ]`\n",
    "  - Final activations (output layer): `[ 0.12, 0.56, 0.23, 0.09, ... ]` (These represent class probabilities for the 10 digit classes).\n",
    "\n",
    "The number of weights between layers depends on the number of neurons in the current and next layers. For example, if you have 128 neurons in the current layer and 64 neurons in the next layer, you will have `128 * 64 = 8,192` weights.\n",
    "\n",
    "The number of activations corresponds to the number of neurons in the layer, not the number of connections (weights). For instance, if you have 128 neurons in the layer, you will have 128 activations.\n",
    "\n",
    "\n",
    "In short, **weights** determine the transformation applied to the input data, while **activations** represent the intermediate results of those transformations that are passed between layers.\n",
    "\n",
    "- **How it Works:** Uses dynamic range information at runtime to scale activations.\n",
    "- **Space Savings:** ~50% (weights only reduced to int8).\n",
    "- **Precision Loss:** Low; suitable for most applications.\n",
    "\n",
    "#### 3. Quantization-Aware Training (QAT)\n",
    "- **What it Does:** Simulates quantization during training to adapt weights for lower precision.\n",
    "- **How it Works:** Introduces fake quantization nodes in the training graph to mimic int8 behavior.\n",
    "In QAT, for instance, a weight value of `0.123` might be simulated as `0.125` during training to mimic the effect of quantization. After training, the model is then quantized to the desired precision (e.g., int8) for deployment.\n",
    "\n",
    "In a standard training process, weights are updated in full precision (e.g., `float32`). However, in QAT, fake quantization nodes are inserted between layers, causing the weights and activations to behave as if they were quantized (e.g., converted to int8 or float16). This means that during forward propagation, the values passed between layers are simulated as quantized values. For example:\n",
    "\n",
    "- The model’s weights might be represented as `0.1234` during normal training, but during QAT, they might behave as `0.125` (simulating rounding).\n",
    "- During backward propagation, gradients are also adjusted with the quantized values in mind.\n",
    "\n",
    "This process helps the model learn to tolerate precision loss and adapt its weights to retain performance when later quantized to a fixed precision (e.g., int8) after training. After training, the weights can be converted to lower precision (e.g., int8), but because the model was trained with quantization effects, it can often retain most of its accuracy despite the lower precision.\n",
    "\n",
    "- **Space Savings:** ~75%.\n",
    "- **Precision Loss:** Minimal; achieves near float32 accuracy if done correctly.\n",
    "\n",
    "#### 4. Per-Tensor Quantization\n",
    "- **What it Does:** Uses a single scaling factor for an entire tensor (layer-wise quantization).\n",
    "- **How it Works:** Scales all weights/activations in a tensor uniformly.\n",
    "- **Space Savings:** Same as static quantization (~75%).\n",
    "- **Precision Loss:** Moderate; less accurate for tensors with wide dynamic ranges.\n",
    "\n",
    "#### 5. Per-Channel Quantization\n",
    "- **What it Does:** Applies different scaling factors per channel in a tensor.\n",
    "- **How it Works:** Improves representation for wide dynamic ranges by scaling each channel separately.\n",
    "- **Space Savings:** ~75%.\n",
    "- **Precision Loss:** Low; better than per-tensor quantization for models with wide dynamic ranges.\n",
    "\n",
    "#### 6. Mixed-Precision Quantization\n",
    "- **What it Does:** Combines multiple precisions (e.g., float16 for activations, int8 for weights).\n",
    "- **How it Works:** Selects precision per layer based on sensitivity and hardware support.\n",
    "- **Space Savings:** ~50%-75%, depending on configuration.\n",
    "- **Precision Loss:** Low; tuned for accuracy-sensitive layers.\n",
    "\n",
    "#### 7. Weight Clustering/Shared Quantization\n",
    "- **What it Does:** Groups weights into clusters and uses a single shared value for each cluster.\n",
    "- **How it Works:** Reduces the number of unique weights, typically followed by lookup tables.\n",
    "- **Space Savings:** Up to 75% or more.\n",
    "- **Precision Loss:** High; depends on the number of clusters.\n",
    "\n",
    "#### 8. Binary Quantization\n",
    "- **What it Does:** Converts weights and activations to binary values (`-1, +1`).\n",
    "- **How it Works:** Uses sign-based representation instead of real-valued weights.\n",
    "- **Space Savings:** ~87.5% (e.g., float32 → 1-bit).\n",
    "- **Precision Loss:** Very High; significant accuracy drop for complex models.\n",
    "\n",
    "#### 9. Ternary Quantization\n",
    "- **What it Does:** Converts weights to ternary values (`-1, 0, +1`).\n",
    "- **How it Works:** Introduces zero weights for sparsity and smaller size.\n",
    "- **Space Savings:** ~85%-90%.\n",
    "- **Precision Loss:** Very High; more accurate than binary quantization but still limited.\n",
    "\n",
    "#### 10. Quantized Embedding Tables\n",
    "- **What it Does:** Quantizes embedding vectors for memory efficiency.\n",
    "- **How it Works:** Uses per-row scaling or lookup tables for embeddings.\n",
    "- **Space Savings:** ~50%-75%.\n",
    "- **Precision Loss:** Low to moderate; depends on embedding size and task.\n",
    "\n",
    "#### 11. Hybrid Quantization\n",
    "- **What it Does:** Quantizes specific parts of the model (e.g., weights) while keeping others (e.g., biases) in higher precision.\n",
    "- **How it Works:** Tailored to balance accuracy and space savings.\n",
    "- **Space Savings:** ~50%-75%.\n",
    "- **Precision Loss:** Low to moderate.\n",
    "\n",
    "#### 12. Stochastic Quantization\n",
    "- **What it Does:** Introduces randomness in quantization by probabilistically rounding values.\n",
    "- **How it Works:** Reduces bias from deterministic rounding errors.\n",
    "- **Space Savings:** Same as static quantization (~75%).\n",
    "- **Precision Loss:** Moderate; slightly better for noisy data.\n",
    "\n",
    "#### 13. Sparsity-Induced Quantization\n",
    "- **What it Does:** Combines pruning and quantization to leverage sparse weights.\n",
    "- **How it Works:** Prunes insignificant weights and quantizes the remaining ones.\n",
    "- **Space Savings:** Up to 90%.\n",
    "- **Precision Loss:** Moderate; sparsity introduces additional accuracy challenges.\n",
    "\n",
    "\n",
    "#### Most Commonly Used Quantization Methods\n",
    "\n",
    "#### 1. **Post-Training Static Quantization**\n",
    "- **Why Common:** Simple to implement and widely supported by tools like TensorFlow Lite and PyTorch.\n",
    "- **Typical Use Case:** Deployment on edge devices where memory and power are constrained.\n",
    "- **Trade-Off:** Moderate precision loss but good balance of accuracy and efficiency.\n",
    "\n",
    "#### 2. **Post-Training Dynamic Range Quantization**\n",
    "- **Why Common:** Easiest to apply without requiring representative datasets for calibration.\n",
    "- **Typical Use Case:** Quick optimization for edge deployment or experimentation.\n",
    "- **Trade-Off:** Lower space savings compared to static quantization, with minimal accuracy loss.\n",
    "\n",
    "#### 3. **Quantization-Aware Training (QAT)**\n",
    "- **Why Common:** Achieves near float32 accuracy with int8 models.\n",
    "- **Typical Use Case:** High-performance, accuracy-critical applications.\n",
    "- **Trade-Off:** Requires retraining, which can be computationally expensive.\n",
    "\n",
    "#### 4. **Per-Channel Quantization**\n",
    "- **Why Common:** Improves accuracy for models with wide dynamic ranges, such as vision models.\n",
    "- **Typical Use Case:** Advanced models for vision or audio tasks.\n",
    "- **Trade-Off:** Slightly more complex implementation but minimal precision loss.\n",
    "- **What it Does:** In per-channel quantization, each channel (e.g., R, G, B for an image) is quantized independently rather than applying the same quantization scale across all channels. This method assigns separate scaling factors to each channel, allowing the model to better handle differences in dynamic range between channels.\n",
    "  \n",
    "**Example with Image Data:**\n",
    "For an image, each pixel typically has 3 values corresponding to the Red (R), Green (G), and Blue (B) channels. In per-channel quantization:\n",
    "- The Red channel may have a dynamic range from 0 to 255.\n",
    "- The Green channel might have a dynamic range from 0 to 200.\n",
    "- The Blue channel may have a dynamic range from 0 to 180.\n",
    "\n",
    "Instead of quantizing the entire pixel with one common scale factor (which might distort the individual channels), each channel is quantized independently with its own scale factor to preserve the unique dynamic range of each.\n",
    "\n",
    "- **Why Common:** It improves the accuracy of models, especially in domains like computer vision, where different channels (like RGB in images) may have very different value ranges.\n",
    "  \n",
    "- **Typical Use Case:** This is widely used in vision models where images consist of multiple channels (e.g., RGB, or even more channels in specialized tasks like multispectral or medical imaging).\n",
    "\n",
    "- **Trade-Off:** Per-channel quantization requires slightly more complex implementation compared to per-tensor quantization (where one scale is used for all channels). However, the performance trade-off is minimal, and accuracy is usually much better since each channel is treated separately based on its characteristics.\n",
    "\n",
    "#### 5. **Mixed-Precision Quantization**\n",
    "- **Why Common:** Supported by modern hardware (e.g., NVIDIA GPUs, TPUs) for optimal efficiency.\n",
    "- **Typical Use Case:** Large-scale models where space and computational efficiency matter.\n",
    "- **Trade-Off:** Requires tuning and hardware compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible Quantization Conversions with Space Savings, Accuracy Loss, and Examples\n",
    "\n",
    "#### 1. **float32 → int8**\n",
    "- **Space Saved:** ~75% (4 bytes → 1 byte per value)\n",
    "- **Accuracy Loss:** Minimal (~1-2%) with proper calibration and representative datasets.\n",
    "- **Range:** int8 values range from -128 to 127.\n",
    "- **Example:**\n",
    "  - float32: `[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "  - int8: `[[ 4, -30, -19, 34, 15 ]]`\n",
    "    **Explanation:** The int8 representation allows for both positive and negative values (i.e., signed integers), making it suitable for data where both positive and negative values are important.\n",
    "\n",
    "\n",
    "#### 2. **float32 → float16**\n",
    "- **Space Saved:** ~50% (4 bytes → 2 bytes per value)\n",
    "- **Accuracy Loss:** Negligible (<1%) for most models.\n",
    "- **Example:**\n",
    "  - float32: `[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "  - float16: `[[ 0.0043, -0.03, -0.019, 0.034, 0.015 ]]`\n",
    "\n",
    "#### 3. **float32 → uint8**\n",
    "- **Space Saved:** ~75% (4 bytes → 1 byte per value)\n",
    "- **Accuracy Loss:** Minimal (~1-2%).\n",
    "- **Range:** uint8 values range from 0 to 255 (only positive values).\n",
    "- **Example:**\n",
    "  - float32: `[[ 0.00431718, 0.03002159, 0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "  - uint8: `[[ 4, 30, 19, 34, 15 ]]`\n",
    "    **Explanation:** The uint8 representation only handles positive values (i.e., unsigned integers), so it's best used for data that does not need to represent negative numbers, such as images with pixel values or other non-negative quantities.\n",
    "\n",
    "#### 4. **float32 → int16**\n",
    "- **Space Saved:** ~50% (4 bytes → 2 bytes per value)\n",
    "- **Accuracy Loss:** Very low (~<1%).\n",
    "- **int16** (signed 16-bit integer) has a range of:\n",
    "  - **Minimum:** -32,768\n",
    "  - **Maximum:** 32,767\n",
    "- **Example:**\n",
    "  - float32: `[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "  - int16: `[[ 43, -300, -193, 343, 155 ]]`\n",
    "\n",
    "#### 5. **float32 → binary (1-bit)**\n",
    "- **Space Saved:** ~97% (4 bytes → 1 bit per value)\n",
    "- **Accuracy Loss:** Significant (~5-10% or higher).\n",
    "- **Example:**\n",
    "  - float32: `[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "  - binary: `[[ 1, -1, -1, 1, 1 ]]`\n",
    "\n",
    "#### 6. **float32 → ternary (2-bit)**\n",
    "- **Space Saved:** ~94% (4 bytes → 2 bits per value)\n",
    "- **Accuracy Loss:** Moderate (~3-5%).\n",
    "- **Example:**\n",
    "  - float32: `[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "  - ternary: `[[ 1, -1, -1, 1, 0 ]]`\n",
    "\n",
    "#### 7. **float16 → int8**\n",
    "- **Space Saved:** ~50% (2 bytes → 1 byte per value)\n",
    "- **Accuracy Loss:** Minimal (~1-2%).\n",
    "- **Example:**\n",
    "  - float16: `[[ 0.0043, -0.03, -0.019, 0.034, 0.015 ]]`\n",
    "  - int8: `[[ 4, -30, -19, 34, 15 ]]`\n",
    "\n",
    "#### 8. **float64 → float32**\n",
    "- **Space Saved:** ~50% (8 bytes → 4 bytes per value)\n",
    "- **Accuracy Loss:** Negligible (<1%).\n",
    "- **Example:**\n",
    "  - float64: `[[ 0.004317182431221, -0.030021592843127, -0.01927675346211, 0.03434618742121, 0.01545171212451 ]]`\n",
    "  - float32: `[[ 0.00431718, -0.03002159, -0.01927675, 0.03434618, 0.0154517 ]]`\n",
    "\n",
    "#### 9. **float64 → int8**\n",
    "- **Space Saved:** ~87.5% (8 bytes → 1 byte per value)\n",
    "- **Accuracy Loss:** High (~5-10% or more).\n",
    "- **Example:**\n",
    "  - float64: `[[ 0.004317182431221, -0.030021592843127, -0.01927675346211, 0.03434618742121, 0.01545171212451 ]]`\n",
    "  - int8: `[[ 4, -30, -19, 34, 15 ]]`\n",
    "\n",
    "#### 10. **int32 → int16**\n",
    "- **Space Saved:** ~50% (4 bytes → 2 bytes per value)\n",
    "- **Accuracy Loss:** Negligible (<1%).\n",
    "- **Example:**\n",
    "  - int32: `[[ 4321, -30021, -19276, 34346, 15545 ]]`\n",
    "  - int16: `[[ 4321, -30021, -19276, 34346, 15545 ]]`\n",
    "\n",
    "#### 11. **int32 → int8**\n",
    "- **Space Saved:** ~75% (4 bytes → 1 byte per value)\n",
    "- **Accuracy Loss:** Minimal (~1-2%).\n",
    "- **Example:**\n",
    "  - int32: `[[ 4321, -30021, -19276, 34346, 15545 ]]`\n",
    "  - int8: `[[ 43, -30, -19, 34, 15 ]]`\n",
    "\n",
    "\n",
    "#### Explanation of Quantization: How It Works, Why It Works, and Usage During Inference\n",
    "\n",
    "#### **How It Was Done**\n",
    "1. **Scaling and Zero Point:**\n",
    "   - **Scaling Factor:** Quantization maps the original floating-point range (e.g., -1 to 1) to an integer range (e.g., -128 to 127 for int8). \n",
    "     - Example: Multiply floating-point weights by a scaling factor (e.g., 1000 for float32 → int8).\n",
    "   - **Zero Point:** A small offset (usually 0 for symmetric quantization) is added to align the integer representation with the floating-point range.\n",
    "\n",
    "   Formula:\n",
    "   $$\\text{quantized\\_value} = \\text{round}(\\text{float\\_value} \\times \\text{scale}) + \\text{zero\\_point}$$\n",
    "\n",
    "2. **Dequantization During Inference:**\n",
    "   - The integer values are scaled back to approximate the original floating-point range using:\n",
    "   $$\\text{float\\_value} \\approx (\\text{quantized\\_value} - \\text{zero\\_point}) / \\text{scale}$$\n",
    "\n",
    "3. **Quantized Model Conversion:**\n",
    "   - During model conversion (e.g., TensorFlow Lite), weights and activations are quantized using the above mapping. The conversion uses calibration datasets to minimize the loss of precision.\n",
    "\n",
    "#### **Why It Works**\n",
    "1. **Neural Networks Are Robust to Small Perturbations:**\n",
    "   - Neural networks tolerate small numerical changes due to their inherent redundancy, especially for weights and activations.\n",
    "   - Quantization reduces precision but retains the overall pattern of the weights.\n",
    "\n",
    "2. **Efficiency in Arithmetic Operations:**\n",
    "   - Integer arithmetic is faster and consumes less power compared to floating-point arithmetic on most hardware.\n",
    "\n",
    "3. **Calibration with Representative Datasets:**\n",
    "   - By calibrating with a representative dataset, the quantization process ensures that the range of weights and activations is well-covered, minimizing the loss in accuracy.\n",
    "\n",
    "#### **How It Is Used During Inference**\n",
    "1. **Pre-Quantized Weights:**\n",
    "   - The model uses the quantized weights and activations directly, stored in integer format.\n",
    "\n",
    "2. **Integer Arithmetic Operations:**\n",
    "   - Most modern hardware accelerators (e.g., Tensor cores, Edge TPUs) support fast matrix multiplication and convolutions using integers, speeding up computations.\n",
    "\n",
    "3. **Dynamic Range Adjustment:**\n",
    "   - During inference, inputs are scaled to match the range of the quantized model using the same scale and zero point applied during quantization.\n",
    "\n",
    "4. **Output Conversion:**\n",
    "   - The final outputs (e.g., logits or probabilities) are dequantized back to floating-point for interpretation.\n",
    "\n",
    "#### **Key Benefits in Inference**\n",
    "- **Reduced Model Size:** Smaller memory footprint due to the use of integers instead of floats.\n",
    "- **Improved Latency:** Faster computations due to optimized integer operations.\n",
    "- **Lower Power Consumption:** Ideal for edge devices like mobile phones and IoT devices.\n",
    "\n",
    "#### **Example of Inference Flow**\n",
    "1. Input image (`float32`) → Quantized to `int8` using scale and zero point.\n",
    "2. `int8` weights and activations processed in the quantized model.\n",
    "3. Final output (`int8`) → Dequantized to `float32` for probabilities/logits.\n",
    "\n",
    "#### **Challenges and Limitations**\n",
    "- **Loss in Accuracy:**\n",
    "  - If the quantization range does not capture outliers effectively, accuracy drops.\n",
    "- **Complexity in Hybrid Models:**\n",
    "  - Some operations may remain in floating-point (e.g., softmax) in hybrid quantization models, reducing efficiency.\n",
    "- **Hardware Support:**\n",
    "  - Quantization works best on hardware designed for integer operations; older hardware may not fully exploit the benefits.\n",
    "\n",
    "This quantization approach balances the trade-offs between model size, speed, and accuracy, making it highly effective for deploying ML models on resource-constrained devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantize(GGUF quants) any model on HuggingFace, you can make use of the following repository: https://huggingface.co/spaces/ggml-org/gguf-my-repo"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is GGML?\n",
    "\n",
    "**GGML (General Graph Machine Learning)** is a lightweight framework for running large language models (LLMs) on resource-constrained hardware (e.g., CPUs). It focuses on efficient inference through techniques like **quantization**.\n",
    "\n",
    "#### GGML Quantization Methods\n",
    "\n",
    "Quantization reduces model size by lowering weight precision. GGML offers several methods:\n",
    "\n",
    "- **Q2, Q3, Q4, Q5, Q6, Q8**: Numbers indicate **bits per weight**. Fewer bits mean smaller size but may reduce accuracy.\n",
    "- **Suffixes**:\n",
    "  - **_K**: Clustering for optimized quantization.\n",
    "  - **_S**: Symmetric quantization.\n",
    "  - **_M**: Mixed precision.\n",
    "  - **_L**: Logarithmic quantization for better range.\n",
    "\n",
    "These methods balance performance, size, and accuracy.\n",
    "\n",
    "#### Are GGML Quantization Methods Related to Graph ML?\n",
    "\n",
    "No, GGML quantization focuses on LLM efficiency and is unrelated to graph-based machine learning.\n",
    "\n",
    "#### GGML Quantization Methods\n",
    "\n",
    "Quantization reduces the precision of numbers in a model (e.g., converting floating-point numbers to integers) to make the model faster and smaller. Below are explanations of the various GGML quantization methods.\n",
    "\n",
    "#### **Quantization Levels (Prefixes)**\n",
    "1. **Q2, Q3, Q4, Q5, Q6, Q8**:\n",
    "   - The number indicates the number of bits per weight.\n",
    "   - Example:\n",
    "     - **Q2**: 2-bit quantization (most aggressive compression, less precision).\n",
    "     - **Q8**: 8-bit quantization (higher precision, less compression).\n",
    "\n",
    "#### **Quantization Suffixes**\n",
    "1. **_0**:\n",
    "   - No additional optimization; straightforward quantization.\n",
    "\n",
    "2. **_K**:\n",
    "   - Uses **k-means clustering** to optimize quantized values, grouping weights into clusters.\n",
    "\n",
    "3. **_S**:\n",
    "   - Employs **symmetric quantization**, where weights are distributed symmetrically around zero.\n",
    "   - Reduces computational complexity but might lose some precision for weights close to zero.\n",
    "\n",
    "4. **_M**:\n",
    "   - Indicates **mixed quantization**, where different parts of the model use different bit widths.\n",
    "   - Aims to balance performance and accuracy.\n",
    "\n",
    "5. **_L**:\n",
    "   - Stands for **logarithmic quantization**, which compresses weights into logarithmic scales for better dynamic range representation.\n",
    "\n",
    "#### **How to Choose?**\n",
    "- **Q2 or Q3**:\n",
    "  - Best for extremely resource-constrained environments where memory is critical.\n",
    "  - Expect some accuracy loss.\n",
    "- **Q4**:\n",
    "  - Balanced trade-off between compression and accuracy.\n",
    "  - Commonly used for deploying large models on edge devices.\n",
    "- **Q5 or Q6**:\n",
    "  - Higher precision with slightly more memory usage.\n",
    "- **Q8**:\n",
    "  - Best for cases where accuracy is critical but memory constraints are less severe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.608164205932617\n",
      "Epoch 2, Loss: 1.5363000733057657\n",
      "Epoch 3, Loss: 1.5200950430552165\n",
      "Float32 Accuracy: 0.9428\n",
      "Int8 Quantized Accuracy: 0.9422\n",
      "\n",
      "Original Float32 Weights (First Linear Layer):\n",
      "tensor([[-0.0013,  0.0422,  0.0640,  ...,  0.0558,  0.0469,  0.0614],\n",
      "        [ 0.0065, -0.0042,  0.0275,  ...,  0.0186,  0.0276, -0.0136],\n",
      "        [-0.0102,  0.0420,  0.0090,  ...,  0.0005, -0.0235,  0.0396],\n",
      "        [ 0.0058,  0.0401,  0.0117,  ..., -0.0233, -0.0271, -0.0185],\n",
      "        [ 0.0034,  0.0239, -0.0205,  ..., -0.0085, -0.0023, -0.0067]])\n",
      "\n",
      "Quantized Weights (First Linear Layer - Int8):\n",
      "tensor([[ 0,  8, 11,  ..., 10,  8, 11],\n",
      "        [ 1, -1,  5,  ...,  3,  5, -2],\n",
      "        [-2,  8,  2,  ...,  0, -4,  7],\n",
      "        [ 1,  7,  2,  ..., -4, -5, -3],\n",
      "        [ 1,  4, -4,  ..., -2,  0, -1]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.quantized as quantized\n",
    "quantized.engine = 'qnnpack'  # or 'fbgemm' based on your system (CPU)\n",
    "### on macbook m1 pro, ran into error RuntimeError: Didn't find engine for operation quantized::linear_prepack NoQEngine\n",
    "### looked around and found the above fix\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "model = MNISTModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "# Evaluate model accuracy on test data\n",
    "def evaluate_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "float32_accuracy = evaluate_model(model, testloader)\n",
    "print(f\"Float32 Accuracy: {float32_accuracy:.4f}\")\n",
    "\n",
    "# Quantization: Convert model to quantized version\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "model_q = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Evaluate quantized model accuracy\n",
    "quantized_accuracy = evaluate_model(model_q, testloader)\n",
    "print(f\"Int8 Quantized Accuracy: {quantized_accuracy:.4f}\")\n",
    "\n",
    "# Save float32 and int8 models\n",
    "torch.save(model.state_dict(), \"data/model_float32.pth\")\n",
    "torch.save(model_q.state_dict(), \"data/model_int8.pth\")\n",
    "\n",
    "# Extract and print weights (for float32 and int8)\n",
    "print(\"\\nOriginal Float32 Weights (First Linear Layer):\")\n",
    "print(model.fc1.weight.data[:5])  # First 5 weights from the first layer\n",
    "\n",
    "# Quantized model weights\n",
    "print(\"\\nQuantized Weights (First Linear Layer - Int8):\")\n",
    "print(model_q.fc1.weight().int_repr()[:5])  # First 5 quantized weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Points:\n",
    "\n",
    "- **`quantize_dynamic`**: This method converts only certain layers (like `nn.Linear` in this case) to the specified dtype (e.g., `torch.qint8`). It performs dynamic quantization, meaning it quantizes the weights on the fly during inference.\n",
    "\n",
    "- **Evaluation Mode**: The model must be in `eval()` mode when applying quantization.\n",
    "\n",
    "- **Corrected Weight Extraction**: The quantized model uses the `int_repr()` function to access the quantized weights. Make sure you're calling it correctly to avoid errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
