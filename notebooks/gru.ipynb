{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is GRU (Gated Recurrent Unit)?\n",
    "Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) that is designed to handle sequential data while addressing the limitations of traditional RNNs, particularly in terms of learning long-term dependencies. GRUs use a gating mechanism to control the flow of information, making them simpler and often faster to train than LSTMs while achieving similar performance.\n",
    "\n",
    "#### Use Cases for GRU\n",
    "- Natural Language Processing (NLP):\n",
    "\n",
    "    - Text generation\n",
    "    - Sentiment analysis\n",
    "    - Language translation\n",
    "-  Time Series Forecasting:\n",
    "\n",
    "    - Stock price prediction\n",
    "    - Weather forecasting\n",
    "- Speech Recognition:\n",
    "\n",
    "    - Converting spoken language into text\n",
    "- Music Generation:\n",
    "\n",
    "    - Composing sequences of music based on learned patterns\n",
    "- Sequence Prediction:\n",
    "\n",
    "    - Any task involving sequential data, such as activity recognition or video classification\n",
    "\n",
    "#### Generate Random Data for GRU\n",
    "We'll create sequential data similar to the previous examples. This data will consist of sequences of numbers where the goal is to predict the next number in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data shape: (1000, 10, 1)\n",
      "Generated target shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to generate sequential data\n",
    "def generate_sequence_data(num_sequences=1000, sequence_length=10):\n",
    "    # Generate random sequences of integers\n",
    "    X = np.random.randint(0, 100, (num_sequences, sequence_length))\n",
    "    # The target is the next number in the sequence\n",
    "    y = np.array([sequence[-1] for sequence in X])\n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_sequence_data()\n",
    "\n",
    "# Reshape X for GRU input (num_samples, time_steps, features)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Generated data shape:\", X.shape)\n",
    "print(\"Generated target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement GRU from Scratch\n",
    "Hereâ€™s a simple implementation of a GRU from scratch using NumPy. This implementation will include the forward pass, loss calculation, and the necessary gates to control information flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: [[-0.38979856  0.43669448 -0.41907663 -0.30644092  0.64495973]]\n",
      "Loss: 5477.202022370382\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize parameters\n",
    "        self.input_size = input_size    # Number of input features\n",
    "        self.hidden_size = hidden_size    # Number of hidden units\n",
    "        self.output_size = output_size    # Number of output features\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.W_z = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # Update gate\n",
    "        self.W_r = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # Reset gate\n",
    "        self.W_h = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # New memory content\n",
    "        \n",
    "        # Bias vectors\n",
    "        self.b_z = np.zeros((1, hidden_size))\n",
    "        self.b_r = np.zeros((1, hidden_size))\n",
    "        self.b_h = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Hidden state\n",
    "        self.h = np.zeros((1, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the GRU.\"\"\"\n",
    "        for t in range(x.shape[1]):\n",
    "            # Combine previous hidden state and current input\n",
    "            combined = np.hstack((self.h, x[:, t, :]))\n",
    "            \n",
    "            # Compute update gate\n",
    "            z_t = self.sigmoid(np.dot(combined, self.W_z) + self.b_z)\n",
    "            # Compute reset gate\n",
    "            r_t = self.sigmoid(np.dot(combined, self.W_r) + self.b_r)\n",
    "            # Compute candidate hidden state\n",
    "            h_hat_t = np.tanh(np.dot(np.hstack((r_t * self.h, x[:, t, :])), self.W_h) + self.b_h)\n",
    "            # Update hidden state\n",
    "            self.h = (1 - z_t) * self.h + z_t * h_hat_t\n",
    "        \n",
    "        # Final output\n",
    "        y = self.h  # Output from the last time step\n",
    "        return y\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Compute the loss using Mean Squared Error.\"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1   # We have a single feature (the number itself)\n",
    "hidden_size = 5  # Number of hidden units\n",
    "output_size = 1  # Predicting the next number\n",
    "\n",
    "# Create GRU model\n",
    "gru = GRU(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass example\n",
    "y_pred = gru.forward(X[0:1])  # Forward pass for the first sequence\n",
    "loss = gru.compute_loss(y_pred, np.array([[y[0]]]))  # Calculate loss\n",
    "print(\"Predicted output:\", y_pred)\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use GRUs\n",
    "- Use GRUs When:\n",
    "\n",
    "    - You are dealing with sequential data where the learning of temporal dependencies is crucial.\n",
    "    - You need a simpler and faster alternative to LSTMs that still captures long-term dependencies effectively.\n",
    "-  Do Not Use GRUs When:\n",
    "\n",
    "    - Your data is not sequential or lacks temporal dependencies.\n",
    "    - You need extremely high accuracy in modeling long-term dependencies where LSTMs may outperform GRUs.\n",
    "#### Loss Function\n",
    "The typical loss function used for training GRUs is Mean Squared Error (MSE) for regression tasks. For classification tasks, Cross-Entropy Loss is commonly used.\n",
    "\n",
    "#### Optimizing the Algorithm\n",
    "To optimize the GRU algorithm:\n",
    "\n",
    "- Gradient Descent: Use optimization algorithms like Adam or RMSProp for effective convergence.\n",
    "- Batch Training: Train using mini-batches instead of individual sequences to stabilize training.\n",
    "- Regularization: Apply dropout or L2 regularization to mitigate overfitting.\n",
    "- Use Pre-trained Models: Consider leveraging pre-trained GRU models for transfer learning on similar tasks.\n",
    "- Hyperparameter Tuning: Optimize the number of hidden units, learning rate, and other hyperparameters through techniques like grid search or random search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
