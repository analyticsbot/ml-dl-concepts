{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is LSTM (Long Short-Term Memory)?\n",
    "Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) architecture specifically designed to learn long-term dependencies in sequence data. LSTMs overcome the limitations of traditional RNNs, such as vanishing gradients, by using a gating mechanism that controls the flow of information. This allows them to retain relevant information over long sequences.\n",
    "\n",
    "#### Use Cases for LSTM\n",
    "- Natural Language Processing (NLP):\n",
    "\n",
    "    - Language translation\n",
    "    - Text generation\n",
    "    - Sentiment analysis\n",
    "\n",
    "-  Time Series Analysis:\n",
    "\n",
    "    - Stock price forecasting\n",
    "    - Weather prediction\n",
    "\n",
    "- Speech Recognition:\n",
    "\n",
    "    - Converting audio signals to text\n",
    "- Music Generation:\n",
    "\n",
    "    - Composing music based on patterns learned from training data\n",
    "- Video Analysis:\n",
    "\n",
    "    - Action recognition and classification in videos\n",
    "\n",
    "#### Generate Random Data for LSTM\n",
    "We’ll generate sequential data suitable for training an LSTM. The data will consist of sequences of numbers where the goal is to predict the next number in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data shape: (1000, 10, 1)\n",
      "Generated target shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to generate sequential data\n",
    "def generate_sequence_data(num_sequences=1000, sequence_length=10):\n",
    "    # Generate random sequences of integers\n",
    "    X = np.random.randint(0, 100, (num_sequences, sequence_length))\n",
    "    # The target is the next number in the sequence\n",
    "    y = np.array([sequence[-1] for sequence in X])\n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_sequence_data()\n",
    "\n",
    "# Reshape X for LSTM input (num_samples, time_steps, features)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Generated data shape:\", X.shape)\n",
    "print(\"Generated target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement LSTM from Scratch\n",
    "Here’s a simple implementation of an LSTM from scratch using NumPy. This will include the forward pass, loss calculation, and the necessary gates to control the information flow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output: [[-0.08452609  0.08048105 -0.16245769 -0.41555806  0.09558229]]\n",
      "Loss: 5490.444131364466\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize parameters\n",
    "        self.input_size = input_size    # Number of input features\n",
    "        self.hidden_size = hidden_size    # Number of hidden units\n",
    "        self.output_size = output_size    # Number of output features\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.W_f = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # Forget gate\n",
    "        self.W_i = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # Input gate\n",
    "        self.W_c = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # Cell gate\n",
    "        self.W_o = np.random.randn(input_size + hidden_size, hidden_size) * 0.01  # Output gate\n",
    "        \n",
    "        # Bias vectors\n",
    "        self.b_f = np.zeros((1, hidden_size))\n",
    "        self.b_i = np.zeros((1, hidden_size))\n",
    "        self.b_c = np.zeros((1, hidden_size))\n",
    "        self.b_o = np.zeros((1, hidden_size))\n",
    "\n",
    "        # Hidden state and cell state\n",
    "        self.h = np.zeros((1, hidden_size))\n",
    "        self.c = np.zeros((1, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the LSTM.\"\"\"\n",
    "        for t in range(x.shape[1]):\n",
    "            # Combine previous hidden state and current input\n",
    "            combined = np.hstack((self.h, x[:, t, :]))\n",
    "            \n",
    "            # Compute forget gate\n",
    "            f_t = self.sigmoid(np.dot(combined, self.W_f) + self.b_f)\n",
    "            # Compute input gate\n",
    "            i_t = self.sigmoid(np.dot(combined, self.W_i) + self.b_i)\n",
    "            # Compute candidate memory cell\n",
    "            c_hat_t = np.tanh(np.dot(combined, self.W_c) + self.b_c)\n",
    "            # Update cell state\n",
    "            self.c = f_t * self.c + i_t * c_hat_t\n",
    "            # Compute output gate\n",
    "            o_t = self.sigmoid(np.dot(combined, self.W_o) + self.b_o)\n",
    "            # Update hidden state\n",
    "            self.h = o_t * np.tanh(self.c)\n",
    "        \n",
    "        # Final output\n",
    "        y = self.h  # Output from the last time step\n",
    "        return y\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Compute the loss using Mean Squared Error.\"\"\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1   # We have a single feature (the number itself)\n",
    "hidden_size = 5  # Number of hidden units\n",
    "output_size = 1  # Predicting the next number\n",
    "\n",
    "# Create LSTM model\n",
    "lstm = LSTM(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass example\n",
    "y_pred = lstm.forward(X[0:1])  # Forward pass for the first sequence\n",
    "loss = lstm.compute_loss(y_pred, np.array([[y[0]]]))  # Calculate loss\n",
    "print(\"Predicted output:\", y_pred)\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use LSTMs\n",
    "Use LSTMs When:\n",
    "\n",
    "- You are dealing with sequential data where long-term dependencies are crucial.\n",
    "- Traditional RNNs struggle to retain information over long sequences due to vanishing gradients.\n",
    "\n",
    "Do Not Use LSTMs When:\n",
    "\n",
    "- The data does not have a sequential nature (e.g., independent data points).\n",
    "- Real-time performance is critical, and the model complexity is a concern since LSTMs can be computationally expensive.\n",
    "\n",
    "#### Loss Function\n",
    "The typical loss function used for training LSTMs is Mean Squared Error (MSE) for regression tasks. For classification tasks, Cross-Entropy Loss is commonly used.\n",
    "\n",
    "#### Optimizing the Algorithm\n",
    "To optimize the LSTM algorithm:\n",
    "\n",
    "- Gradient Descent: Use optimization algorithms like Adam or RMSProp for effective convergence.\n",
    "- Batch Training: Train using mini-batches instead of individual sequences to stabilize training.\n",
    "- Regularization: Apply dropout or L2 regularization to mitigate overfitting.\n",
    "- Use Pre-trained Models: Consider leveraging pre-trained LSTM models for transfer learning on similar tasks.\n",
    "- Hyperparameter Tuning: Optimize the number of hidden units, learning rate, and other hyperparameters through techniques like grid search or random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
