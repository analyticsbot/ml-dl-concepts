{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model Types and Scenarios\n",
    "\n",
    "#### A. Model Types:\n",
    "- Regression and Classification Models: Traditional ML models predicting continuous values or class labels.\n",
    "- Matrix Factorization (MF) Models for Recommendation: Collaborative filtering approach to recommend items by decomposing user-item interaction matrices.\n",
    "- Collaborative Filtering (CF) Models for Recommendation: Based on user behavior, recommendations are made based on similar users or items.\n",
    "- Two-Tower Models for Recommendation: Typically deep learning architectures that learn user and item embeddings separately to generate recommendations.\n",
    "- Large Language Models (LLMs): Models capable of understanding and generating human-like text, often used for NLP tasks.\n",
    "\n",
    "#### B. Deployment Scenarios:\n",
    "- Batch: Process data in bulk at scheduled intervals.\n",
    "- Near-Real Time: Process data with low latency, usually seconds to minutes after the data arrives.\n",
    "- Edge: Deploy models directly on edge devices for local processing.\n",
    "\n",
    "#### 2. System Design using Various Technologies\n",
    "#### A. Google Cloud Functions\n",
    "\n",
    "- Overview: Serverless function service that executes code in response to events (HTTP triggers, Pub/Sub).\n",
    "- Components:\n",
    "    - Event-driven architecture\n",
    "    - Autoscaling based on incoming requests\n",
    "- When to Use: For lightweight tasks that require low latency and can benefit from autoscaling.\n",
    "- Limitations: Cold starts can lead to latency, limited execution time (max 9 minutes), not suitable for long-running processes.\n",
    "\n",
    "#### B. Docker\n",
    "- Overview: Containerization platform that allows packaging applications and their dependencies into isolated containers.\n",
    "- Components:\n",
    "    - Docker Engine\n",
    "    - Docker Hub (or any other registry)\n",
    "    - Docker Compose for multi-container applications\n",
    "- When to Use: When needing to ensure consistency across environments (development, testing, production).\n",
    "- Limitations: Requires orchestration tools for scalability, resource-heavy compared to serverless solutions.\n",
    "\n",
    "#### C. Kubernetes (K8s)\n",
    "- Overview: Container orchestration platform for automating deployment, scaling, and management of containerized applications.\n",
    "- Components:\n",
    "    - Pods, Nodes, Deployments, Services, ConfigMaps, etc.\n",
    "- When to Use: For applications that require high availability, scalability, and complex orchestration.\n",
    "- Limitations: Steeper learning curve, management overhead compared to simpler services.\n",
    "\n",
    "#### D. Kubeflow\n",
    "- Overview: Machine learning toolkit for Kubernetes, providing components for building ML pipelines.\n",
    "- Components:\n",
    "    - Pipelines, Katib (hyperparameter tuning), KFServing (model serving).\n",
    "- When to Use: For managing end-to-end ML workflows on Kubernetes.\n",
    "- Limitations: Requires Kubernetes knowledge, can be complex for smaller projects.\n",
    "\n",
    "#### E. Google Cloud Run\n",
    "- Overview: Fully managed serverless platform for running containerized applications.\n",
    "- Components:\n",
    "    - Automatically scales based on traffic\n",
    "    - Supports any language/runtime\n",
    "    - When to Use: For deploying stateless applications with autoscaling and minimal management.\n",
    "    - Limitations: May not be ideal for applications with persistent state or complex orchestration needs.\n",
    "\n",
    "#### F. Amazon ECS/Fargate\n",
    "- Overview: Container orchestration services for running Docker containers on AWS.\n",
    "- Components:\n",
    "    - ECS orchestrates Docker containers\n",
    "    - Fargate allows for serverless execution of containers without managing servers.\n",
    "- When to Use: When deploying applications on AWS that require containerization.\n",
    "- Limitations: AWS lock-in, and potential complexity in managing clusters.\n",
    "\n",
    "#### 3. Orchestration for Model Training and Deployment\n",
    "\n",
    "#### A. Model Training and Registry\n",
    "- Periodic Training: Use orchestration tools like Apache Airflow or Kubernetes CronJobs to schedule model retraining jobs.\n",
    "- Model Registry: Use tools like MLflow or TensorFlow Model Registry to version and manage trained models.\n",
    "- Validation Before Pushing: Implement a CI/CD pipeline with automated tests (unit tests, integration tests) to validate model performance before deployment. Use metrics like accuracy, precision, recall, and RMSE.\n",
    "\n",
    "#### B. Online Learning and Updates\n",
    "- Online Learning: Implement models that can adapt to new data as it arrives, using techniques such as:\n",
    "- Incremental learning algorithms that can be updated without retraining from scratch.\n",
    "- Use of streaming data platforms like Apache Kafka or Google Pub/Sub to ingest and process data in real-time.\n",
    "- Regular Updates: Set up a feedback loop where the model can periodically retrain based on new data or user interactions.\n",
    "\n",
    "\n",
    "#### 4. Summary of Deployment Strategies\n",
    "| Technology | Best Use Case | Limitations |\n",
    "|---|---|---|\n",
    "| Google Cloud Functions | Event-driven, lightweight tasks | Cold starts, limited execution time |\n",
    "| Docker | Consistency across environments | Requires orchestration for scalability |\n",
    "| Kubernetes | High availability, complex applications | Steep learning curve |\n",
    "| Kubeflow | End-to-end ML workflows | Requires K8s knowledge, complexity |\n",
    "| Google Cloud Run | Stateless applications with autoscaling | Not ideal for persistent state |\n",
    "| Amazon ECS/Fargate | Docker containers on AWS | AWS lock-in, potential management complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Considerations for Model Deployment Strategies\n",
    "\n",
    "#### 1. Model Versioning\n",
    "- Importance: Keep track of different model versions, especially when deploying updated models. This is critical for rollback scenarios if the new model performs poorly.\n",
    "- Implementation: Use tools like MLflow, DVC, or custom version control systems to manage and store model versions alongside their metadata.\n",
    "\n",
    "#### 2. Model Explainability and Interpretability\n",
    "- Importance: Especially in sectors like finance and healthcare, understanding how models make predictions is essential for trust and regulatory compliance.\n",
    "- Tools: Consider using libraries like SHAP or LIME for model interpretability, which can help in understanding feature importance and model decisions.\n",
    "\n",
    "#### 3. Data Drift and Concept Drift Handling\n",
    "- Monitoring: Regularly monitor data distribution and model performance to detect when the model may no longer perform well due to shifts in data (data drift) or changes in the underlying relationships (concept drift).\n",
    "- Techniques: Implement automated systems to trigger model retraining based on drift detection, using statistical tests or monitoring metrics like AUC or F1 score.\n",
    "\n",
    "#### 4. Multi-Model Strategies\n",
    "- Ensemble Learning: Consider deploying multiple models (e.g., voting classifiers or stacking) to improve prediction accuracy and robustness.\n",
    "- Model Selection: Implement a mechanism to select the best-performing model based on real-time metrics or periodic evaluations.\n",
    "\n",
    "#### 5. Infrastructure as Code (IaC)\n",
    "- Automation: Use IaC tools like Terraform or AWS CloudFormation to automate the provisioning and management of cloud infrastructure, ensuring reproducibility and consistency.\n",
    "- Version Control: Keep infrastructure code in version control alongside application code to track changes over time.\n",
    "\n",
    "#### 6. Cost Monitoring and Optimization\n",
    "- Budgeting: Regularly review cloud spending and usage patterns. Services like Google Cloud’s Cost Management tools can help identify areas for cost reduction.\n",
    "- Auto-Scaling: Use auto-scaling features in Kubernetes or cloud services to dynamically adjust resources based on load, ensuring you’re not over-provisioning resources.\n",
    "\n",
    "#### 7. Integration with CI/CD Pipelines\n",
    "- Continuous Integration/Continuous Deployment: Integrate model training and deployment into CI/CD pipelines using tools like Jenkins, GitLab CI/CD, or GitHub Actions to automate testing and deployment processes.\n",
    "- Automated Testing: Include unit tests and performance tests for models in the CI pipeline to ensure that only models meeting performance criteria are deployed.\n",
    "\n",
    "#### 8. Security and Compliance\n",
    "- Data Protection: Implement encryption for data at rest and in transit. Use access controls and identity management solutions to limit access to sensitive data.\n",
    "- Compliance: Ensure that deployment strategies comply with industry regulations (e.g., GDPR, HIPAA) by implementing audit trails and data handling policies.\n",
    "\n",
    "#### 9. User Feedback Loop\n",
    "- Active Learning: Implement a feedback mechanism to gather user interactions and feedback on model predictions, allowing models to learn from user behavior and improve over time.\n",
    "\n",
    "#### 10. Documentation and Training\n",
    "- Knowledge Sharing: Document model deployment processes, architecture decisions, and performance metrics to facilitate knowledge sharing among team members and ensure smooth transitions during handoffs or onboarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design a scalable ML system\n",
    "\n",
    "To design a scalable real-time ML system with auto-scaling, re-training, and concurrency, we'll break down the requirements and architecture. I’ll also provide a high-level diagram of the architecture to illustrate how the components interact.\n",
    "\n",
    "#### System Requirements\n",
    "- Real-Time Inference: Low latency predictions with requests handled concurrently.\n",
    "- Auto-Scaling: Scale the inference service based on incoming requests to ensure optimal resource usage.\n",
    "- Non-blocking Design: Ensure that requests are handled independently and do not block each other.\n",
    "- Periodic Re-training: Automate the re-training pipeline based on data drift, scheduled intervals, or new data availability.\n",
    "- Continuous Monitoring: Monitor performance, request rates, error rates, and system health.\n",
    "- Model Registry: Keep track of model versions to deploy updated models seamlessly.\n",
    "- Model Deployment and Rollback: Quickly deploy new models and roll back if needed.\n",
    "\n",
    "\n",
    "#### Architecture Components\n",
    "- API Gateway: Routes incoming requests and handles initial traffic distribution.\n",
    "- Load Balancer: Balances traffic across multiple inference instances to avoid bottlenecks.\n",
    "- Inference Service: Containerized or serverless model that performs predictions. Scales up or down based on demand.\n",
    "- Data Stream (Optional): For real-time data ingestion (e.g., Kafka, Pub/Sub) to handle continuous input of new data points.\n",
    "- Feature Store: Stores pre-processed features to ensure consistent features are used across training and inference.\n",
    "- Model Training Pipeline: Scheduled or triggered based on data drift detection. The pipeline includes data extraction, feature engineering, model training, evaluation, and registration.\n",
    "- Model Registry: Version control for models to manage deployments and rollbacks.\n",
    "- Orchestrator (e.g., Kubeflow): Automates workflows for training and deployment, ensuring smooth model management.\n",
    "- Monitoring and Alerting: Tracks model performance, resource utilization, and data drift.\n",
    "\n",
    "#### System Architecture Diagram\n",
    "Here's a high-level architecture diagram. Each number corresponds to a component explained in the following sections.\n",
    "\n",
    "                               ┌──────────────────────┐\n",
    "                               │      Clients        │\n",
    "                               └──────────────────────┘\n",
    "                                         │\n",
    "                                         ▼\n",
    "                                 ┌─────────────┐\n",
    "                                 │ API Gateway │\n",
    "                                 └─────────────┘\n",
    "                                         │\n",
    "                              ┌──────────┴──────────┐\n",
    "                              ▼                     ▼\n",
    "                     ┌──────────────────┐    ┌──────────────────┐\n",
    "                     │  Load Balancer   │    │  Monitoring &    │\n",
    "                     └──────────────────┘    │    Logging       │\n",
    "                              │              └──────────────────┘\n",
    "                              ▼\n",
    "                     ┌──────────────────┐\n",
    "                     │ Inference Service│  ⟶  (Autoscaling)\n",
    "                     └──────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                  ┌─────────────────────────────┐\n",
    "                  │        Feature Store        │\n",
    "                  └─────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                     ┌───────────────────┐\n",
    "                     │  Data Pipeline    │\n",
    "                     └───────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                  ┌─────────────────────────────┐\n",
    "                  │ Model Training Pipeline     │\n",
    "                  │ (Data Prep, Model Training) │\n",
    "                  └─────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                     ┌──────────────────┐\n",
    "                     │ Model Registry   │\n",
    "                     └──────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                     ┌───────────────────────┐\n",
    "                     │    Orchestrator       │\n",
    "                     └───────────────────────┘\n",
    "\n",
    "\n",
    "#### Component Details\n",
    "- API Gateway:\n",
    "\n",
    "    - Purpose: Acts as the entry point for all incoming requests and routes them to the inference service.\n",
    "    - Design: Use a managed API gateway (e.g., Google Cloud Endpoints or AWS API Gateway) for easy scalability, caching, and security.\n",
    "    - Benefits: Allows the system to handle high volumes of requests and ensures each request is handled independently, achieving non-blocking behavior.\n",
    "- Load Balancer:\n",
    "\n",
    "    - Purpose: Distributes requests evenly across multiple instances of the inference service.\n",
    "    - Design: Use Cloud Load Balancing (e.g., GCP Load Balancer, AWS Elastic Load Balancer) to ensure the inference service can scale with the load, allowing smooth request handling and reliability.\n",
    "\n",
    "- Inference Service:\n",
    "\n",
    "    - Deployment: Deploy the model in Docker containers on Kubernetes or serverless (e.g., Google Cloud Run for stateless apps).\n",
    "    - Scaling: Set auto-scaling policies to scale up or down based on CPU/memory usage or request load.\n",
    "    - Benefits: Containers allow easy model updates without downtime and support multiple versions for A/B testing or blue-green deployment.\n",
    "\n",
    "- Data Stream (Optional):\n",
    "\n",
    "    - Purpose: For applications needing continuous data flow (like time-series or clickstream data), a streaming service (e.g., Kafka or Google Pub/Sub) enables processing new data points in real-time.\n",
    "    - Integration: This data can feed into the training pipeline, triggering re-training or adaptation for up-to-date models.\n",
    "\n",
    "- Feature Store:\n",
    "\n",
    "    - Purpose: Ensures the same features are used in training and real-time inference for consistency.\n",
    "    - Examples: Feast (Feature Store for ML) can be used for scalable, real-time access to features.\n",
    "    - Benefits: Reduces data leakage and drift, ensuring feature consistency across different model versions.\n",
    "\n",
    "- Model Training Pipeline:\n",
    "\n",
    "    - Components: Automated pipeline that retrieves data, preprocesses it, trains the model, and evaluates performance.\n",
    "    - Design: Use Kubeflow Pipelines or Google AI Platform Pipelines for automation. Configure to re-train on a schedule or when data drift is detected.\n",
    "    - Benefits: Ensures continuous improvement of models by periodically retraining and using the latest data.\n",
    "\n",
    "- Model Registry:\n",
    "\n",
    "    - Purpose: Stores model versions, metadata, and performance metrics.\n",
    "    - Examples: MLflow or Google Model Registry can help manage versioned models for deployment.\n",
    "    - Benefits: Version control and easy rollback in case of performance degradation.\n",
    "\n",
    "- Orchestrator:\n",
    "\n",
    "    - Purpose: Manages end-to-end workflows, including retraining, version control, and deployment.\n",
    "    - Examples: Kubeflow orchestrates ML workflows on Kubernetes, while Google Cloud Composer (based on Apache Airflow) can handle complex dependency scheduling.\n",
    "    - Benefits: Centralized management of the ML lifecycle ensures reliability and streamlined processes.\n",
    "\n",
    "- Monitoring and Alerting:\n",
    "\n",
    "    - Purpose: Tracks system health (e.g., latency, errors), model performance (accuracy, drift), and resource utilization.\n",
    "    - Examples: Prometheus and Grafana for metrics, ELK stack for logs, Cloud Monitoring for alerts.\n",
    "    - Benefits: Alerts can trigger auto-scaling or initiate model re-training, ensuring the system remains performant and efficient.\n",
    "\n",
    "#### Orchestration and Model Updates\n",
    "- Pipeline Automation:\n",
    "\n",
    "    - Orchestration: Configure the orchestrator (e.g., Kubeflow) to trigger a re-training pipeline periodically or when data drift is detected.\n",
    "    - Versioning and Registry: Upon successful evaluation, register the new model version in the model registry.\n",
    "\n",
    "- Deployment and Rollback:\n",
    "\n",
    "    - CI/CD Integration: Use CI/CD pipelines to automate the deployment of new models, ensuring new models meet performance benchmarks.\n",
    "    - A/B Testing: Deploy new models alongside the existing version to compare performance on live traffic.\n",
    "    - Rollback Mechanism: If new models underperform, the registry allows an instant rollback to previous versions.\n",
    "- Auto-Scaling and Concurrency Management:\n",
    "\n",
    "    - Concurrency: Use Kubernetes’ horizontal pod autoscaler to scale inference pods based on CPU/memory usage, ensuring concurrent request handling.\n",
    "    - Non-blocking: Each inference request is handled by separate instances or threads, ensuring independence and minimal latency.\n",
    "\n",
    "- Online Learning (Optional):\n",
    "\n",
    "    - Incremental Training: For cases requiring real-time updates, use online learning techniques that adapt the model based on new data without full retraining.\n",
    "    - Warm Starts: Initialize new training cycles with weights from the current model for faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoscaling is a critical feature in machine learning systems, especially for real-time inference, where traffic can vary widely depending on user demand, seasonality, or business needs. It ensures that resources scale up to meet high traffic demands and scale down during low demand, optimizing both performance and cost. Let’s explore how autoscaling works, its types, and best practices in the context of a real-time ML system.\n",
    "\n",
    "#### Key Aspects of Autoscaling\n",
    "- Load-based Scaling:\n",
    "\n",
    "    - Autoscaling responds to real-time traffic load (CPU, memory, or request count). This ensures the system can handle increased requests without latency or performance degradation.\n",
    "    - For inference workloads, CPU and GPU usage are common metrics, with models requiring more resources as the number of concurrent requests increases.\n",
    "\n",
    "- Predictive Scaling:\n",
    "\n",
    "    - Predictive or proactive scaling uses machine learning or historical data patterns to anticipate spikes in demand before they happen.\n",
    "    - For example, a retail platform may experience higher traffic during sales events, holidays, or weekends. Predictive scaling can pre-provision resources based on these patterns to avoid bottlenecks.\n",
    "- Scheduled Scaling:\n",
    "\n",
    "    - Scheduled scaling involves predefined scaling policies based on expected periods of high or low demand. For instance, you can set a schedule to increase the number of instances during typical peak hours.\n",
    "\n",
    "#### Types of Autoscaling\n",
    "- Horizontal Autoscaling (Scale Out/In):\n",
    "\n",
    "    - Adds or removes instances (e.g., additional containers or VMs) based on demand.\n",
    "    - Works well with stateless applications, like model inference, where each instance can handle requests independently.\n",
    "    - Supported by orchestration tools like Kubernetes, which automatically adjusts the number of pods (containers) based on load.\n",
    "\n",
    "- Vertical Autoscaling (Scale Up/Down):\n",
    "\n",
    "    - Increases or decreases the resources (CPU, memory, GPU) allocated to an existing instance rather than adding more instances.\n",
    "    - Useful for applications that cannot easily be distributed across multiple instances or for databases where splitting instances can be challenging.\n",
    "    - Vertical scaling is limited by the hardware capacity of the machine and can result in some downtime as resources are reallocated.\n",
    "\n",
    "#### Autoscaling in Kubernetes\n",
    "Kubernetes offers robust support for autoscaling through several built-in options:\n",
    "\n",
    "- Horizontal Pod Autoscaler (HPA):\n",
    "\n",
    "    - Automatically scales the number of pods in a deployment or replica set based on observed CPU utilization or custom metrics.\n",
    "    - Configurable to set minimum and maximum thresholds, so the number of pods automatically adjusts within those bounds.\n",
    "    - Works well for real-time ML inference applications where requests per second (RPS) and CPU are good indicators of load.\n",
    "- Cluster Autoscaler:\n",
    "\n",
    "    - Scales the Kubernetes cluster itself by adding or removing nodes to meet the resource needs of the scheduled pods.\n",
    "    - Ensures the infrastructure dynamically adapts to the workload and avoids over-provisioning.\n",
    "    - Commonly used in cloud environments where infrastructure can scale out to handle additional loads.\n",
    "\n",
    "- Vertical Pod Autoscaler (VPA):\n",
    "\n",
    "    - Adjusts the CPU and memory requests of pods over time based on historical usage patterns, without creating new pods.\n",
    "    - Helps maintain optimal resource usage without under- or over-allocating resources to pods.\n",
    "\n",
    "#### Autoscaling Considerations for Real-Time ML Systems\n",
    "\n",
    "- Latency Sensitivity:\n",
    "\n",
    "    - For real-time inference, scaling decisions should aim to minimize latency and avoid queuing delays. HPA can be configured to scale before latency thresholds are breached.\n",
    "    - Monitoring incoming traffic patterns allows for setting autoscaling thresholds based on latency and request count metrics.\n",
    "\n",
    "- Cold Start Times:\n",
    "\n",
    "    - In serverless environments (e.g., AWS Lambda, Google Cloud Functions), scaling can lead to cold starts, where instances take time to initialize, adding latency.\n",
    "    - Solutions include keeping a small number of \"warm\" instances running or using predictive scaling to reduce the cold start impact.\n",
    "\n",
    "- Cost Optimization:\n",
    "\n",
    "    - Autoscaling avoids paying for resources when demand is low, but it’s important to set appropriate upper limits to prevent runaway costs during unexpected traffic spikes.\n",
    "    - It’s also valuable to configure cooldown periods to avoid the \"ping-pong\" effect (frequent scaling up and down).\n",
    "\n",
    "- Monitoring and Custom Metrics:\n",
    "\n",
    "    - Beyond CPU and memory, using custom metrics like inference request count, queue length, or request latency can improve autoscaling accuracy.\n",
    "    - Monitoring these metrics with tools like Prometheus, Grafana, or native cloud monitoring solutions enables data-driven autoscaling decisions.\n",
    "\n",
    "- Regional Scaling:\n",
    "\n",
    "    - If traffic patterns are region-specific, deploying in multiple regions with autoscaling ensures that latency stays low and load is balanced.\n",
    "    - Kubernetes allows autoscaling within a region, and tools like Google Cloud Load Balancer or AWS Global Accelerator can distribute traffic globally.\n",
    "\n",
    "#### Example Configuration of Autoscaling in a Real-Time ML System\n",
    "Let’s consider a Kubernetes setup for a real-time inference model where:\n",
    "\n",
    "- Horizontal Pod Autoscaler (HPA) is set to add more pods if CPU utilization exceeds 70%.\n",
    "- Cluster Autoscaler adds nodes when the pod demand exceeds available resources.\n",
    "\n",
    "Example Policy:\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2beta2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: inference-service-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: inference-service\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 20\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```\n",
    "\n",
    "In this example:\n",
    "\n",
    "The inference service starts with 2 pods and can scale up to 20.\n",
    "Pods will be added if CPU utilization goes beyond 70%, ensuring that the system adapts as requests increase, optimizing latency and throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
