{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder Algorithm\n",
    "An autoencoder is a type of neural network used for unsupervised learning. Its primary goal is to learn a compressed representation (encoding) of input data and then reconstruct the original input from this representation. An autoencoder consists of two main parts:\n",
    "\n",
    "- Encoder: This part transforms the input into a lower-dimensional space (latent space).\n",
    "- Decoder: This part reconstructs the input from the encoded representation.\n",
    "\n",
    "#### Use Cases for Autoencoders\n",
    "- Dimensionality Reduction: Autoencoders can reduce the number of features in a dataset while preserving important information, similar to PCA (Principal Component Analysis).\n",
    "\n",
    "- Image Denoising: Autoencoders can learn to reconstruct clean images from noisy inputs, effectively removing noise.\n",
    "\n",
    "- Anomaly Detection: By training on normal data, an autoencoder can identify anomalies based on reconstruction error (i.e., the difference between the input and output).\n",
    "\n",
    "- Data Generation: Variational Autoencoders (VAEs), a type of autoencoder, can be used to generate new data points similar to the training data.\n",
    "\n",
    "- Recommender Systems: Autoencoders can be applied to collaborative filtering, where user-item interactions are used to predict missing values.\n",
    "\n",
    "- Feature Extraction: Autoencoders can learn useful representations from raw data, which can be fed into other machine learning models.\n",
    "\n",
    "#### Generating Random Data for Autoencoder\n",
    "For demonstration purposes, we can generate synthetic data such as a set of images or continuous features. Here’s a simple example of generating random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating random data: 1000 samples, each with 20 features\n",
    "num_samples = 1000\n",
    "num_features = 20\n",
    "\n",
    "# Random data from a normal distribution\n",
    "random_data = np.random.randn(num_samples, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Autoencoder from Scratch Using NumPy\n",
    "Here’s a simple implementation of an autoencoder using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: [[0.50655112 0.50599847 0.5052421  ... 0.50576718 0.50569644 0.50639328]\n",
      " [0.50655599 0.50601819 0.50523885 ... 0.50579837 0.50571565 0.50638851]\n",
      " [0.50653172 0.50599728 0.50524021 ... 0.50577184 0.50571479 0.50637256]\n",
      " ...\n",
      " [0.50670992 0.50612963 0.50535345 ... 0.50590624 0.50582235 0.50653656]\n",
      " [0.50663805 0.50608414 0.50530956 ... 0.50585463 0.50577507 0.50647756]\n",
      " [0.50673055 0.50616673 0.50537507 ... 0.50593457 0.5058651  0.50655901]]\n",
      "Epoch 100, Loss: [[0.00157314 0.02655879 0.36584953 ... 0.00395815 0.01225422 0.00130747]\n",
      " [0.0075067  0.013991   0.003592   ... 0.00523662 0.11580369 0.00180653]\n",
      " [0.00806494 0.04224086 0.00157517 ... 0.00942657 0.88305723 0.00232079]\n",
      " ...\n",
      " [0.03117858 0.00789503 0.43707013 ... 0.15439402 0.00810393 0.04210065]\n",
      " [0.00543724 0.00102111 0.31464857 ... 0.01775521 0.13919244 0.00290764]\n",
      " [0.02411799 0.31234257 0.96369125 ... 0.05471066 0.03743378 0.03582218]]\n",
      "Epoch 200, Loss: [[1.47182226e-03 3.58434376e-03 7.25096811e-01 ... 1.25599218e-02\n",
      "  1.20473599e-02 3.81732360e-04]\n",
      " [2.59865353e-02 2.42927759e-03 2.90733492e-03 ... 5.32849134e-03\n",
      "  4.63309376e-02 9.79182332e-04]\n",
      " [4.27253626e-02 1.98120746e-02 4.05521164e-04 ... 1.11270301e-01\n",
      "  8.54102531e-01 5.88872736e-04]\n",
      " ...\n",
      " [3.67627562e-02 7.59119796e-03 1.28413417e-01 ... 4.19460581e-01\n",
      "  6.43097004e-04 2.41243455e-02]\n",
      " [3.53489677e-03 2.78584054e-04 2.33413783e-01 ... 1.15882646e-01\n",
      "  1.27248612e-01 8.21452038e-04]\n",
      " [4.36015721e-02 4.96982862e-01 9.86230882e-01 ... 3.44668665e-02\n",
      "  1.51526458e-02 2.13396329e-02]]\n",
      "Epoch 300, Loss: [[5.11223988e-04 2.53823972e-03 6.89205485e-01 ... 5.27408139e-03\n",
      "  5.97161000e-03 1.43548680e-04]\n",
      " [2.02255316e-02 3.19744282e-03 1.80349598e-03 ... 1.02775715e-03\n",
      "  1.92151855e-02 5.37105505e-04]\n",
      " [3.70347135e-02 1.58668759e-02 1.00294573e-04 ... 9.15691985e-02\n",
      "  8.20481940e-01 2.29576654e-04]\n",
      " ...\n",
      " [3.69739763e-02 6.09871412e-03 4.58901980e-02 ... 3.24781654e-01\n",
      "  2.55623456e-04 1.60275222e-02]\n",
      " [1.07159523e-03 1.27335629e-04 1.42979002e-01 ... 1.26921911e-01\n",
      "  9.79258527e-02 3.44451112e-04]\n",
      " [5.53057569e-02 6.37172737e-01 9.88857838e-01 ... 1.15340579e-02\n",
      "  1.10095125e-02 1.65348263e-02]]\n",
      "Epoch 400, Loss: [[4.04853569e-04 1.82566158e-03 5.71990291e-01 ... 2.01159851e-03\n",
      "  1.79200320e-03 7.59515084e-05]\n",
      " [9.17303108e-03 1.07365028e-03 1.09093270e-03 ... 1.57943207e-04\n",
      "  1.73986677e-02 2.46963977e-04]\n",
      " [6.25057023e-02 4.55098372e-03 2.15528692e-05 ... 9.61971551e-02\n",
      "  9.68038284e-01 1.18997529e-04]\n",
      " ...\n",
      " [4.80987996e-02 5.96064147e-03 1.83238454e-02 ... 2.15736663e-01\n",
      "  6.68496659e-05 1.30385638e-02]\n",
      " [5.58351511e-04 8.27518847e-05 4.61368166e-02 ... 6.28221678e-02\n",
      "  3.05445853e-02 2.68117521e-04]\n",
      " [7.18675820e-02 6.92784049e-01 9.90893533e-01 ... 2.21102757e-03\n",
      "  3.38395124e-02 1.19975709e-02]]\n",
      "Epoch 500, Loss: [[1.61905889e-04 7.65646422e-04 5.61997490e-01 ... 1.40856739e-03\n",
      "  1.34658105e-03 7.12773667e-05]\n",
      " [5.15104140e-03 8.10040937e-04 8.18869165e-04 ... 8.54529071e-05\n",
      "  7.46170900e-03 1.94783327e-04]\n",
      " [5.60340496e-02 2.77709563e-03 1.90621104e-05 ... 1.64984264e-01\n",
      "  9.86047594e-01 8.09150804e-05]\n",
      " ...\n",
      " [6.79806253e-02 8.95402698e-03 3.85166479e-03 ... 1.77702461e-01\n",
      "  1.88500657e-05 1.15411851e-02]\n",
      " [1.69671688e-04 5.69977978e-05 2.82956933e-02 ... 8.26366860e-02\n",
      "  2.36447378e-02 2.39199247e-04]\n",
      " [9.50317072e-02 8.20484586e-01 9.92775069e-01 ... 2.72258795e-04\n",
      "  3.87292761e-02 8.38468253e-03]]\n",
      "Epoch 600, Loss: [[1.50011096e-04 2.60594336e-03 4.97578507e-01 ... 2.20552286e-03\n",
      "  8.43369535e-04 5.67949095e-05]\n",
      " [4.18497472e-03 6.63697937e-04 7.04828870e-04 ... 7.12095458e-05\n",
      "  4.96730892e-03 1.62694403e-04]\n",
      " [7.09067524e-02 2.16331622e-03 6.74149253e-06 ... 1.19656886e-01\n",
      "  9.91321618e-01 5.82797750e-05]\n",
      " ...\n",
      " [4.80359046e-02 4.87123900e-03 4.08915789e-03 ... 1.61290762e-01\n",
      "  1.28363386e-05 1.00677509e-02]\n",
      " [6.99705282e-05 4.39197612e-05 2.55012571e-02 ... 7.99370102e-02\n",
      "  1.20325182e-02 2.27128318e-04]\n",
      " [1.25701634e-01 8.86587290e-01 9.94351094e-01 ... 1.47902864e-04\n",
      "  3.31326798e-02 7.67036460e-03]]\n",
      "Epoch 700, Loss: [[9.84232383e-05 2.40120033e-03 4.78334286e-01 ... 1.80921529e-03\n",
      "  5.67245004e-04 4.89851694e-05]\n",
      " [4.16030197e-03 7.16664774e-04 4.96320352e-04 ... 6.98493871e-05\n",
      "  3.39958748e-03 1.34435537e-04]\n",
      " [4.90027673e-02 2.58976418e-03 7.04836529e-06 ... 1.07295051e-01\n",
      "  9.86293011e-01 4.27419205e-05]\n",
      " ...\n",
      " [3.88939828e-02 5.58502275e-03 3.46718748e-03 ... 1.54799723e-01\n",
      "  8.75199280e-06 9.08005977e-03]\n",
      " [3.16176429e-05 2.86519697e-05 2.13645247e-02 ... 7.01712114e-02\n",
      "  6.42832155e-03 2.10495465e-04]\n",
      " [1.86718113e-01 9.11870369e-01 9.95797371e-01 ... 9.67102369e-05\n",
      "  3.27592161e-02 6.98591783e-03]]\n",
      "Epoch 800, Loss: [[6.02607564e-05 8.67190827e-04 1.23550463e-01 ... 2.65699334e-03\n",
      "  3.68702507e-04 3.94618080e-05]\n",
      " [2.50140034e-03 3.04198652e-04 2.85662961e-04 ... 2.97683556e-05\n",
      "  1.41991919e-03 1.21495720e-04]\n",
      " [2.44324741e-02 1.94009360e-03 4.41512211e-06 ... 1.87620352e-01\n",
      "  9.86079943e-01 3.28759364e-05]\n",
      " ...\n",
      " [1.94820476e-02 2.22250117e-02 5.41489356e-02 ... 1.90365934e-01\n",
      "  8.26894172e-06 1.42842634e-02]\n",
      " [1.26062890e-05 8.57476787e-06 2.36867337e-02 ... 1.12197986e-01\n",
      "  5.67188555e-03 1.92580568e-04]\n",
      " [2.39429252e-01 9.15877788e-01 9.96973935e-01 ... 4.04541877e-05\n",
      "  2.51243522e-02 6.19420217e-03]]\n",
      "Epoch 900, Loss: [[3.37079906e-05 2.60073337e-04 1.10321729e-01 ... 1.62254767e-03\n",
      "  1.58097642e-04 3.95553536e-05]\n",
      " [1.87584126e-03 1.95934174e-04 4.17896791e-04 ... 1.76399711e-05\n",
      "  2.70529296e-04 1.19241299e-04]\n",
      " [4.76397173e-03 6.57409272e-03 1.16161478e-05 ... 3.98803637e-02\n",
      "  1.89338277e-01 2.54097712e-05]\n",
      " ...\n",
      " [1.25122056e-02 1.03873748e-02 6.94428436e-02 ... 3.19587992e-01\n",
      "  3.15691956e-05 1.01065692e-02]\n",
      " [5.23717243e-06 1.87523233e-06 3.82377275e-02 ... 2.97078919e-01\n",
      "  1.08969255e-02 1.33458097e-04]\n",
      " [3.42311188e-01 9.32239764e-01 9.97386333e-01 ... 7.54543348e-05\n",
      "  1.44446162e-02 6.48397923e-03]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Weights initialization\n",
    "        self.weights_encoder = np.random.rand(input_size, hidden_size) * 0.01\n",
    "        self.weights_decoder = np.random.rand(hidden_size, input_size) * 0.01\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.encoded = self.sigmoid(np.dot(x, self.weights_encoder))\n",
    "        self.decoded = self.sigmoid(np.dot(self.encoded, self.weights_decoder))\n",
    "        return self.decoded\n",
    "\n",
    "    def backward(self, x):\n",
    "        # Calculate loss (Mean Squared Error)\n",
    "        loss = x - self.decoded\n",
    "        \n",
    "        # Backpropagation\n",
    "        d_decoder = loss * self.sigmoid_derivative(self.decoded)\n",
    "        d_encoder = np.dot(d_decoder, self.weights_decoder.T) * self.sigmoid_derivative(self.encoded)\n",
    "\n",
    "        # Update weights\n",
    "        self.weights_decoder += np.dot(self.encoded.T, d_decoder) * self.learning_rate\n",
    "        self.weights_encoder += np.dot(x.T, d_encoder) * self.learning_rate\n",
    "\n",
    "        return np.mean(np.square(loss))\n",
    "\n",
    "    def fit(self, x, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.forward(x)\n",
    "            self.backward(x)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Example usage\n",
    "autoencoder = Autoencoder(input_size=20, hidden_size=10)\n",
    "autoencoder.fit(random_data, epochs=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to Use Autoencoder and When Not to Use It\n",
    "- When to Use Autoencoders:\n",
    "\n",
    "    - When you have large amounts of unlabeled data.\n",
    "    - For tasks like anomaly detection, where reconstruction error can indicate outliers.\n",
    "    - For dimensionality reduction, especially when dealing with high-dimensional data.\n",
    "    - When you need to learn data representations that can be useful for other tasks.\n",
    "- When Not to Use Autoencoders:\n",
    "\n",
    "    - When you have a small amount of labeled data; supervised learning methods might be more effective.\n",
    "    - For tasks where the reconstruction error is not meaningful (e.g., certain classification tasks).\n",
    "    - When interpretability is crucial, as autoencoders can be more challenging to interpret than simpler models.\n",
    "\n",
    "#### What is the Loss Function\n",
    "The most common loss function used in autoencoders is Mean Squared Error (MSE), which measures the average squared difference between the original input and the reconstructed output. It is defined as:\n",
    "\n",
    "$ MSE = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{x}_i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* $x_i$: Original input value\n",
    "* $\\hat{x}_i$: Reconstructed output value\n",
    "* $n$: Number of samples\n",
    "\n",
    "#### How to Optimize the Algorithm\n",
    "- Hyperparameter Tuning: Experiment with different sizes of the hidden layer, learning rates, and batch sizes to find optimal values.\n",
    "\n",
    "- Regularization: Implement techniques like L1 or L2 regularization to prevent overfitting.\n",
    "\n",
    "- Batch Normalization: This can help stabilize learning by normalizing the inputs to each layer.\n",
    "\n",
    "- Different Activation Functions: Experiment with various activation functions (e.g., ReLU, Tanh) to see what works best for your data.\n",
    "\n",
    "- Advanced Architectures: Consider using more advanced types of autoencoders, such as Variational Autoencoders (VAEs) or Denoising Autoencoders, to improve performance for specific tasks.\n",
    "\n",
    "- Early Stopping: Monitor the loss during training and stop when it no longer improves to avoid overfitting.\n",
    "\n",
    "- Optimization Algorithms: Use advanced optimization algorithms like Adam or RMSprop instead of standard gradient descent to improve convergence speed and stability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
