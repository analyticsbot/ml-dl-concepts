{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a Convolutional Neural Network (CNN) from scratch involves understanding various concepts and components used in training deep learning models. Below, Iâ€™ll outline the implementation of a simple CNN using NumPy, along with explanations of key concepts used in training such a model.\n",
    "\n",
    "#### Key Concepts in CNN Training\n",
    "- Convolutional Layers: These layers apply convolution operations to the input to extract features. Each convolutional layer consists of a set of learnable filters (kernels).\n",
    "\n",
    "- Activation Functions: Functions like ReLU (Rectified Linear Unit) introduce non-linearity in the model, allowing it to learn complex patterns.\n",
    "\n",
    "- Pooling Layers: These layers downsample the feature maps, reducing dimensionality and helping to retain important features. Max pooling and average pooling are common methods.\n",
    "\n",
    "- Dropout: A regularization technique that randomly drops a fraction of neurons during training, which helps prevent overfitting.\n",
    "\n",
    "- Layer Normalization: This technique normalizes the output of a layer, stabilizing and accelerating the training process.\n",
    "\n",
    "- Batch Normalization: Similar to layer normalization but applied to mini-batches of data, helping in reducing internal covariate shift.\n",
    "\n",
    "- Group Normalization: A normalization technique that divides the channels into groups and normalizes each group separately, which can be useful in small batch sizes.\n",
    "\n",
    "- Loss Function: The function used to measure the difference between predicted and actual values. For classification tasks, cross-entropy loss is commonly used.\n",
    "\n",
    "- Optimization Algorithm: Techniques like Stochastic Gradient Descent (SGD), Adam, etc., are used to minimize the loss function by updating the weights.\n",
    "\n",
    "#### Implementing a Simple CNN from Scratch\n",
    "Below is a simplified implementation of a CNN for image classification using NumPy. This example will cover the basic structure, but keep in mind that it may not include advanced features like dropout or normalization for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters  # Number of filters to apply\n",
    "        self.filter_size = filter_size  # Size of each filter\n",
    "        # Initialize filters with small random values\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size) * 0.1\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input  # Store the input for potential backpropagation\n",
    "        # Calculate output dimensions based on input and filter size\n",
    "        self.output = np.zeros((self.num_filters, input.shape[1] - self.filter_size + 1, input.shape[2] - self.filter_size + 1))\n",
    "        \n",
    "        # Apply each filter across the input\n",
    "        for i in range(self.num_filters):\n",
    "            for j in range(self.output.shape[1]):\n",
    "                for k in range(self.output.shape[2]):\n",
    "                    # Perform convolution operation: element-wise multiplication followed by summation\n",
    "                    self.output[i, j, k] = np.sum(self.input[:, j:j+self.filter_size, k:k+self.filter_size] * self.filters[i])        \n",
    "        return self.output\n",
    "\n",
    "class MaxPoolingLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size  # Size of the pooling window\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input  # Store the input for potential backpropagation\n",
    "        # Calculate output dimensions after pooling\n",
    "        self.output = np.zeros((input.shape[0], input.shape[1] // self.pool_size, input.shape[2] // self.pool_size))\n",
    "\n",
    "        # Apply max pooling operation\n",
    "        for i in range(input.shape[0]):\n",
    "            for j in range(0, input.shape[1], self.pool_size):\n",
    "                for k in range(0, input.shape[2], self.pool_size):\n",
    "                    # Take the maximum value from the pooling window\n",
    "                    self.output[i, j//self.pool_size, k//self.pool_size] = np.max(input[i, j:j+self.pool_size, k:k+self.pool_size])\n",
    "        return self.output\n",
    "\n",
    "class FlattenLayer:\n",
    "    def forward(self, input):\n",
    "        self.input = input  # Store the input for potential backpropagation\n",
    "        # Flatten the input to match the fully connected layer input requirement\n",
    "        return input.reshape(-1)\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases for the fully connected layer\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.bias = np.zeros(output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input  # Store the input for potential backpropagation\n",
    "        # Compute the output of the fully connected layer using dot product and adding bias\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "# Modified CNN Training Function with Dynamic Flattening\n",
    "def train_cnn(X, y, num_epochs=10):\n",
    "    conv_layer = ConvLayer(num_filters=8, filter_size=3)  # Initialize convolutional layer\n",
    "    pool_layer = MaxPoolingLayer(pool_size=2)  # Initialize max pooling layer\n",
    "    flatten_layer = FlattenLayer()  # Initialize flatten layer\n",
    "    \n",
    "    # Perform a forward pass to determine flattened size\n",
    "    sample_conv_out = conv_layer.forward(X[0:1])  # Forward pass on a single sample\n",
    "    sample_pooled_out = pool_layer.forward(sample_conv_out)  # Apply pooling\n",
    "    # Determine flatten size dynamically\n",
    "    flattened_size = flatten_layer.forward(sample_pooled_out).shape[0]  \n",
    "\n",
    "    # Initialize fully connected layer with dynamic size\n",
    "    fc_layer = FullyConnectedLayer(input_size=flattened_size, output_size=10)  # Assuming 10 output classes\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(len(X)):\n",
    "            # Forward pass through each layer\n",
    "            conv_out = conv_layer.forward(X[i:i+1])  # Convolution layer output\n",
    "            pooled_out = pool_layer.forward(conv_out)  # Pooling layer output\n",
    "            flat_out = flatten_layer.forward(pooled_out)  # Flattened output\n",
    "            output = fc_layer.forward(flat_out)  # Fully connected layer output\n",
    "            \n",
    "            # Normally, calculate loss and backpropagation here...\n",
    "            # For simplicity, we skip those steps in this example\n",
    "\n",
    "# Dummy data for testing\n",
    "X = np.random.rand(10, 28, 28)  # 10 samples of 28x28 images\n",
    "y = np.random.randint(0, 10, 10)  # Random labels for 10 classes\n",
    "\n",
    "train_cnn(X, y)  # Train the CNN with dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the Code\n",
    "- Convolution Layer: Performs the convolution operation. It initializes filters randomly and applies them to the input during the forward pass.\n",
    "\n",
    "- Max Pooling Layer: Reduces the spatial dimensions of the input while retaining important features by taking the maximum value in each pool region.\n",
    "\n",
    "- Flatten Layer: Reshapes the pooled output to be fed into the fully connected layer.\n",
    "\n",
    "- Fully Connected Layer: Connects every input neuron to every output neuron, producing the final output.\n",
    "\n",
    "- Training Loop: In the train_cnn function, a basic training loop is established to demonstrate the forward pass through the network.\n",
    "\n",
    "#### Additional Concepts\n",
    "- Dropout: Implement a dropout layer that randomly sets a fraction of the input units to 0 during training, reducing overfitting.\n",
    "\n",
    "- Normalization: Implement layer normalization or batch normalization layers to stabilize and speed up training.\n",
    "\n",
    "- Backpropagation: Implement the backpropagation algorithm to update weights based on the loss function.\n",
    "\n",
    "- Optimizer: Use optimizers like Adam or SGD for more effective training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
